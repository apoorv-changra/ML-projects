{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apoorv-changra/ML-projects/blob/main/Neural_network_optimisation_using_genetic_algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Genetic Algorithm in Artificial Neural Network and its Optimization Methods**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fnq_aAZ8qDkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to solve a diabetes classification problem using an Artificial Neural Network (ANN) optimized by a Genetic Algorithm, discovering the performance difference of different parameters of the ANN model and comparing this training method with additional optimizers like stochastic gradient descent, RMSprop, and Adam optimizer.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fZFVG3gFqh6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inputs and Outputs**"
      ],
      "metadata": {
        "id": "ttgHCIG3rPQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the provided diabetes dataset (diabetes.txt), there are 11 columns: the first column represents the bias with all values equal to one; The second to the ninth columns are values of features; The 10th column is the label column which indicates whether the instance is classified as diabetes or not.\n",
        "\n",
        "During training, the input for all ANNs will consist of the bias and all 8 features together with the label value (y), while the output of the ANNs should be the predicted output according to the ANNâ€™s computation. The training label (y) is to help the ANN to tune its weights in each layer to compute a predicted output that is as close as the training label (y). As for the inputs and outputs of the layers in the ANN model, starting from the input layer, all outputs from each layer will become the inputs for the next layer (Here, we are building a Sequential model)."
      ],
      "metadata": {
        "id": "V8WD6DPprU6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### diabetes.txt file"
      ],
      "metadata": {
        "id": "ptnklgxZuJUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "lOv9rH2UsEYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014d87f5-31ce-483d-a4f1-876d78526714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your text file\n",
        "file_path = \"diabetes.txt\"\n",
        "\n",
        "# Open the file and read its contents\n",
        "with open(file_path, \"r\") as file:\n",
        "    file_contents = file.read()\n",
        "\n",
        "# Display the contents\n",
        "print(file_contents)\n"
      ],
      "metadata": {
        "id": "LA9uYDO2sGmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0aa629-23b3-42c0-d8c4-c6675ac6d8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t0.176471\t0.605\t0.42623\t0\t0\t0.536513\t0.0209223\t0.0666667\t1\t0\n",
            "1\t0.352941\t0.72\t0.590164\t0.27\t0.269504\t0.505216\t0.0755764\t0.316667\t0\t1\n",
            "1\t0.117647\t0.875\t0.721311\t0\t0\t0.341282\t0.105892\t0.0166667\t0\t1\n",
            "1\t0.705882\t0.605\t0.639344\t0.17\t0\t0.394933\t0.0772844\t0.683333\t0\t1\n",
            "1\t0.117647\t0.535\t0.606557\t0.3\t0.118203\t0.500745\t0.139197\t0.0333333\t0\t1\n",
            "1\t0.882353\t0.68\t0.57377\t0.32\t0.130024\t0.552906\t0.0320239\t0.366667\t1\t0\n",
            "1\t0.470588\t0.545\t0.622951\t0.39\t0.134752\t0.415797\t0.239966\t0.166667\t1\t0\n",
            "1\t0.117647\t0.405\t0.491803\t0.22\t0\t0.412817\t0.0905209\t0.0666667\t0\t1\n",
            "1\t0.235294\t0.985\t0.57377\t0.39\t0.879433\t0.546945\t0.961144\t0.166667\t0\t1\n",
            "1\t0\t0.525\t0.737705\t0\t0\t0.441133\t0.0508113\t0.416667\t0\t1\n",
            "1\t0.411765\t0.545\t0.655738\t0.31\t0\t0.535022\t0.447908\t0.366667\t1\t0\n",
            "1\t0.235294\t0.415\t0.704918\t0.19\t0\t0.436662\t0.10205\t0.216667\t0\t1\n",
            "1\t0.352941\t0.77\t0.606557\t0.32\t0.228132\t0.436662\t0.324936\t0.3\t0\t1\n",
            "1\t0.176471\t0.79\t0.622951\t0.36\t0.289598\t0.470939\t0.33006\t0.116667\t1\t0\n",
            "1\t0.647059\t0.69\t0.622951\t0\t0\t0.494784\t0.146029\t0.233333\t0\t1\n",
            "1\t0.176471\t0.5\t0.557377\t0.23\t0.0957447\t0.470939\t0.371904\t0.116667\t0\t1\n",
            "1\t0.176471\t0.855\t0.590164\t0.33\t0.159574\t0.496274\t0.0516652\t0.05\t1\t0\n",
            "1\t0.235294\t0.77\t0.508197\t0.31\t0.335697\t0.488823\t0.0678907\t0.0333333\t0\t1\n",
            "1\t0.0588235\t0.435\t0.639344\t0.27\t0.0378251\t0.515648\t0.00982067\t0.0166667\t0\t1\n",
            "1\t0.0588235\t0.485\t0.57377\t0.15\t0\t0.271237\t0.029462\t0\t0\t1\n",
            "1\t0.411765\t0.51\t0.606557\t0.4\t0.124113\t0.554396\t0.0538002\t0.4\t0\t1\n",
            "1\t0.176471\t0.845\t0.606557\t0.19\t0.147754\t0.445604\t0.0811272\t0.166667\t1\t0\n",
            "1\t0.470588\t0.63\t0.606557\t0.38\t0.0886525\t0.385991\t0.0358668\t0.3\t0\t1\n",
            "1\t0.470588\t0.325\t0.590164\t0.23\t0\t0.4769\t0.222886\t0.35\t0\t1\n",
            "1\t0.588235\t0.575\t0\t0\t0\t0\t0.0781383\t0.15\t1\t0\n",
            "1\t0.117647\t0.42\t0\t0\t0\t0\t0.0964987\t0\t0\t1\n",
            "1\t0.0588235\t0.555\t0.770492\t0\t0\t0.488823\t0.0798463\t0.4\t0\t1\n",
            "1\t0\t0.545\t0.721311\t0.3\t0\t0.484352\t0.331768\t0.283333\t1\t0\n",
            "1\t0.0588235\t0.465\t0.57377\t0.31\t0\t0.453055\t0.101196\t0.0333333\t0\t1\n",
            "1\t0\t0.605\t0.540984\t0.3\t0.195035\t0.511177\t0.0533732\t0.2\t1\t0\n",
            "1\t0.117647\t0.42\t0.409836\t0.23\t0.0898345\t0.453055\t0.380017\t0\t0\t1\n",
            "1\t0.352941\t0.74\t0.590164\t0.35\t0\t0.500745\t0.234415\t0.483333\t1\t0\n",
            "1\t0\t0.69\t0.491803\t0.35\t0.1974\t0.515648\t0.194705\t0\t1\t0\n",
            "1\t0.0588235\t0.53\t0.57377\t0.28\t0.159574\t0.509687\t0.0273271\t0.0166667\t0\t1\n",
            "1\t0\t0.505\t0.508197\t0\t0\t0.326379\t0.110162\t0.0666667\t0\t1\n",
            "1\t0.117647\t0.64\t0.52459\t0.42\t0\t0.596125\t0.436806\t0.05\t0\t1\n",
            "1\t0.411765\t0.57\t0.52459\t0\t0\t0.408346\t0.279249\t0.216667\t1\t0\n",
            "1\t0.176471\t0.6\t0.57377\t0.3\t0.159574\t0.639344\t0.159693\t0.15\t0\t1\n",
            "1\t0.235294\t0.71\t0.704918\t0\t0\t0.655738\t0.242101\t0.0166667\t1\t0\n",
            "1\t0.176471\t0.64\t0.590164\t0.25\t0.224586\t0.482861\t0.20111\t0.1\t1\t0\n",
            "1\t0.235294\t0.915\t0\t0\t0\t0.423249\t0.0572161\t0.25\t1\t0\n",
            "1\t0.0588235\t0.435\t0.491803\t0.37\t0.0886525\t0.554396\t0.184031\t0.0166667\t0\t1\n",
            "1\t0.352941\t0.49\t0.47541\t0.33\t0.224586\t0.506706\t0.150299\t0.366667\t0\t1\n",
            "1\t0.0588235\t0.735\t0.770492\t0.41\t0\t0.734724\t0.119556\t0.1\t1\t0\n",
            "1\t0.0588235\t0.675\t0.442623\t0\t0\t0.397914\t0.260034\t0.683333\t0\t1\n",
            "1\t0.0588235\t0.655\t0.52459\t0.14\t0.490544\t0.353204\t0.132792\t0\t0\t1\n",
            "1\t0.588235\t0.645\t0.622951\t0.28\t0.144208\t0.535022\t0.0862511\t0.3\t0\t1\n",
            "1\t0\t0.81\t0.622951\t0.56\t0.118203\t0.792846\t0.290777\t0.0666667\t1\t0\n",
            "1\t0.176471\t0.44\t0.47541\t0.11\t0.0638298\t0.369598\t0.0807003\t0.0166667\t0\t1\n",
            "1\t0.0588235\t0.405\t0.606557\t0.41\t0.0673759\t0.690015\t0.434671\t0.183333\t0\t1\n",
            "1\t0\t0.475\t0.655738\t0.45\t0.108747\t0.543964\t0.1076\t0.0833333\t0\t1\n",
            "1\t0.235294\t0.615\t0.655738\t0.15\t0.208038\t0.4769\t0.15585\t0.216667\t0\t1\n",
            "1\t0\t0.825\t0.737705\t0.33\t0.803783\t0.779434\t0.149018\t0.0333333\t0\t1\n",
            "1\t0\t0.895\t0.737705\t0.27\t0\t0.657228\t0.259607\t0.0333333\t1\t0\n",
            "1\t0.176471\t0.41\t0.57377\t0\t0\t0.314456\t0.132792\t0.0666667\t0\t1\n",
            "1\t0\t0.9\t0.639344\t0.63\t0.0165485\t0.885246\t1\t0.0666667\t1\t0\n",
            "1\t0\t0.335\t0.622951\t0\t0\t0.675112\t0.0495303\t0.416667\t0\t1\n",
            "1\t0\t0.625\t0.786885\t0\t0\t0.33532\t0.0785653\t0\t0\t1\n",
            "1\t0.117647\t0.46\t0.508197\t0.28\t0\t0.470939\t0.0222032\t0.05\t0\t1\n",
            "1\t0.529412\t0.445\t0.508197\t0\t0\t0.33532\t0.0273271\t0.2\t0\t1\n",
            "1\t0.529412\t0.85\t0.606557\t0.31\t0\t0.655738\t0.13877\t0.366667\t1\t0\n",
            "1\t0.235294\t0.515\t0.491803\t0.33\t0.22695\t0.357675\t0.379163\t0.2\t0\t1\n",
            "1\t0.235294\t0.645\t0.491803\t0.12\t0.27305\t0.409836\t0.191716\t0.166667\t0\t1\n",
            "1\t0\t0.525\t0.688525\t0\t0\t0.415797\t0.283091\t0.683333\t1\t0\n",
            "1\t0.176471\t0.865\t0.688525\t0.33\t0.560284\t0.532042\t0.0768574\t0.0166667\t1\t0\n",
            "1\t0.117647\t0.645\t0\t0\t0\t0.57377\t0.0964987\t0.333333\t0\t1\n",
            "1\t0.117647\t0.495\t0.57377\t0.16\t0.0520095\t0.304024\t0.0670367\t0.1\t0\t1\n",
            "1\t0.0588235\t0.45\t0.508197\t0.12\t0.0508274\t0.405365\t0.214347\t0.05\t0\t1\n",
            "1\t0.411765\t0.795\t0.540984\t0\t0\t0.453055\t0.130231\t0.25\t1\t0\n",
            "1\t0.529412\t0.78\t0.704918\t0\t0\t0.369598\t0.0649018\t0.533333\t1\t0\n",
            "1\t0.0588235\t0.365\t0.409836\t0.1\t0\t0.342772\t0.0725875\t0\t0\t1\n",
            "1\t0\t0.39\t0.721311\t0.29\t0.0472813\t0.549925\t0.152007\t0\t0\t1\n",
            "1\t0.411765\t0.53\t0.754098\t0.18\t0\t0.338301\t0.0670367\t0.45\t0\t1\n",
            "1\t0\t0.505\t0.52459\t0.17\t0\t0.312966\t0.0742955\t0\t0\t1\n",
            "1\t0.0588235\t0.4\t0.45082\t0\t0\t0.28465\t0.0768574\t0\t0\t1\n",
            "1\t0\t0.825\t0.622951\t0.43\t0.301418\t0.71386\t0.0772844\t0.0833333\t0\t1\n",
            "1\t0.176471\t0.865\t0.672131\t0.48\t0.549645\t0.57228\t0.879163\t0.0666667\t1\t0\n",
            "1\t0.117647\t0.45\t0.57377\t0.17\t0\t0.406855\t0.0029889\t0.0166667\t0\t1\n",
            "1\t0.235294\t0.47\t0.532787\t0.22\t0\t0.368107\t0.029889\t0\t0\t1\n",
            "1\t0.176471\t0.79\t0.52459\t0.13\t0.457447\t0.464978\t0.0926558\t0.05\t0\t1\n",
            "1\t0.294118\t0.83\t0.622951\t0\t0\t0.681073\t0.11187\t0.1\t1\t0\n",
            "1\t0.235294\t0.55\t0.622951\t0.2\t0.118203\t0.423249\t0.0170794\t0.1\t0\t1\n",
            "1\t0.0588235\t0.84\t0.721311\t0.29\t0\t0.52161\t0.353117\t0.516667\t1\t0\n",
            "1\t0.294118\t0.58\t0.606557\t0\t0\t0.38152\t0.0525192\t0.15\t0\t1\n",
            "1\t0.0588235\t0.445\t0.196721\t0.19\t0.0295508\t0.414307\t0.20538\t0\t0\t1\n",
            "1\t0.411765\t0.685\t0.737705\t0.41\t0\t0.4769\t0.133646\t0.3\t0\t1\n",
            "1\t0.176471\t0.65\t0.52459\t0\t0\t0.344262\t0.100769\t0.0166667\t0\t1\n",
            "1\t0\t0.465\t0.491803\t0\t0\t0.52608\t0.0789923\t0.0666667\t0\t1\n",
            "1\t0\t0.51\t0.639344\t0.4\t0.106383\t0.514158\t0.0683177\t0.05\t0\t1\n",
            "1\t0.294118\t0.44\t0.540984\t0.21\t0.0271868\t0.363636\t0.112724\t0.15\t0\t1\n",
            "1\t0.0588235\t0.46\t0.508197\t0.25\t0.0484634\t0.290611\t0.172502\t0.0666667\t0\t1\n",
            "1\t0.0588235\t0.465\t0.459016\t0.11\t0\t0.33532\t0.144748\t0.0166667\t0\t1\n",
            "1\t0.0588235\t0.72\t0.672131\t0.46\t0.212766\t0.687034\t0.109735\t0.416667\t1\t0\n",
            "1\t0.470588\t0.985\t0.606557\t0\t0\t0.385991\t0.475235\t0.3\t1\t0\n",
            "1\t0.294118\t0.575\t0.622951\t0\t0\t0.464978\t0.113151\t0.383333\t1\t0\n",
            "1\t0.352941\t0.495\t0.491803\t0.19\t0.0638298\t0.400894\t0.178907\t0.183333\t0\t1\n",
            "1\t0.235294\t0.38\t0.508197\t0\t0\t0.506706\t0.133646\t0.0666667\t0\t1\n",
            "1\t0.176471\t0.935\t0.57377\t0.22\t0.236407\t0.542474\t0.140905\t0.25\t1\t0\n",
            "1\t0.0588235\t0.45\t0.557377\t0.08\t0\t0.365127\t0.452605\t0.25\t0\t1\n",
            "1\t0.176471\t0.56\t0.606557\t0.3\t0\t0.470939\t0.0508113\t0.0666667\t1\t0\n",
            "1\t0\t0.63\t0.688525\t0.29\t0.254137\t0.457526\t0.188728\t0.05\t0\t1\n",
            "1\t0.235294\t0.735\t0.606557\t0.25\t0.346336\t0.520119\t0.131085\t0.15\t0\t1\n",
            "1\t0.117647\t0.55\t0.606557\t0.29\t0.147754\t0.482861\t0.264731\t0.1\t0\t1\n",
            "1\t0\t0.47\t0.57377\t0.27\t0.135934\t0.648286\t0.114859\t0\t0\t1\n",
            "1\t0\t0.595\t0\t0\t0\t0.482861\t0.0269001\t0.05\t1\t0\n",
            "1\t0\t0.67\t0.47541\t0.2\t0.343972\t0.393443\t0.116994\t0\t0\t1\n",
            "1\t0.235294\t0.485\t0.491803\t0.23\t0\t0.420268\t0.15585\t0.0166667\t0\t1\n",
            "1\t0.411765\t0.75\t0.639344\t0.29\t0.148936\t0.52459\t0.262169\t0.55\t1\t0\n",
            "1\t0.0588235\t0.485\t0.540984\t0.15\t0.165485\t0.345753\t0.174637\t0.0166667\t0\t1\n",
            "1\t0.294118\t0.545\t0.508197\t0.41\t0.152482\t0.533532\t0.186166\t0.0666667\t1\t0\n",
            "1\t0.529412\t0.36\t0.639344\t0.25\t0\t0.470939\t0.0862511\t0.283333\t0\t1\n",
            "1\t0.235294\t0.67\t0.590164\t0\t0\t0.354694\t0.0849701\t0.65\t1\t0\n",
            "1\t0.352941\t0.595\t0.409836\t0.22\t0.208038\t0.403875\t0.529462\t0.2\t1\t0\n",
            "1\t0.764706\t0.38\t0.491803\t0\t0\t0.488823\t0.0435525\t0.333333\t0\t1\n",
            "1\t0.588235\t0.665\t0.557377\t0\t0\t0.402385\t0.0713066\t0.25\t0\t1\n",
            "1\t0\t0.465\t0.491803\t0.25\t0.108747\t0.42772\t0.193851\t0.0166667\t0\t1\n",
            "1\t0.764706\t0.725\t0.672131\t0.19\t0.130024\t0.330849\t0.0713066\t0.6\t0\t1\n",
            "1\t0.235294\t0.61\t0.557377\t0\t0\t0.52161\t0.134927\t0.133333\t0\t1\n",
            "1\t0.0588235\t0.545\t0.47541\t0.18\t0.137116\t0.424739\t0.060205\t0.0166667\t0\t1\n",
            "1\t0.588235\t0.575\t0\t0\t0\t0.52608\t0.0239112\t0.133333\t0\t1\n",
            "1\t0.529412\t0.51\t0.622951\t0.37\t0\t0.490313\t0.25064\t0.416667\t1\t0\n",
            "1\t0.352941\t0.48\t0\t0\t0\t0.353204\t0.0478224\t0.116667\t0\t1\n",
            "1\t0.176471\t0.53\t0.442623\t0.21\t0.186761\t0.460507\t0.0913749\t0.05\t0\t1\n",
            "1\t0.117647\t0.435\t0\t0.23\t0\t0.4307\t0.296755\t0.0666667\t0\t1\n",
            "1\t0.470588\t0.425\t0.45082\t0.2\t0\t0.363636\t0.0247652\t0.35\t0\t1\n",
            "1\t0.352941\t0.975\t0.57377\t0\t0\t0.460507\t0.106746\t0.166667\t1\t0\n",
            "1\t0.0588235\t0.905\t0.639344\t0.42\t0.346336\t0.596125\t0.503843\t0.0166667\t1\t0\n",
            "1\t0.294118\t0.365\t0.491803\t0\t0\t0.399404\t0.0811272\t0.1\t0\t1\n",
            "1\t0.705882\t0.53\t0.655738\t0\t0\t0.351714\t0.0251921\t0.383333\t0\t1\n",
            "1\t0.117647\t0.455\t0.508197\t0\t0\t0.406855\t0.190863\t0.0166667\t0\t1\n",
            "1\t0.0588235\t0.535\t0.557377\t0.19\t0\t0.394933\t0.0371477\t0.05\t0\t1\n",
            "1\t0.470588\t0.5\t0.622951\t0\t0\t0.576751\t0.0478224\t0.35\t0\t1\n",
            "1\t0.0588235\t0.62\t0.491803\t0.32\t0\t0.533532\t0.186166\t0\t0\t1\n",
            "1\t0.117647\t0.495\t0.491803\t0.17\t0.189125\t0.545455\t0.16012\t0\t0\t1\n",
            "1\t0.294118\t0.39\t0.393443\t0\t0\t0.502235\t0.245944\t0.0666667\t0\t1\n",
            "1\t0.470588\t0.475\t0.590164\t0\t0\t0.548435\t0.173783\t0.6\t0\t1\n",
            "1\t0.0588235\t0.995\t0.622951\t0.43\t0\t0.639344\t0.561913\t0.0166667\t1\t0\n",
            "1\t0.0588235\t0.355\t0.393443\t0.18\t0.0898345\t0.304024\t0.104611\t0.0166667\t0\t1\n",
            "1\t0.0588235\t0.65\t0.491803\t0.23\t0.200946\t0.42623\t0.262169\t0\t0\t1\n",
            "1\t0.0588235\t0.48\t1\t0\t0\t0.33383\t0.0550811\t0.1\t0\t1\n",
            "1\t0.411765\t0.905\t0.688525\t0.21\t0.22695\t0.535022\t0.216909\t0.5\t1\t0\n",
            "1\t0.117647\t0.34\t0.508197\t0.13\t0.0177305\t0.299553\t0.0764304\t0.0333333\t0\t1\n",
            "1\t0.470588\t0.525\t0.819672\t0.36\t0\t0.645306\t0.0687447\t0.4\t1\t0\n",
            "1\t0.294118\t0.545\t0.614754\t0.26\t0\t0.536513\t0.199829\t0.65\t0\t1\n",
            "1\t0.294118\t0.475\t0.590164\t0.33\t0\t0.561848\t0.12468\t0.1\t0\t1\n",
            "1\t0.352941\t0.46\t0.754098\t0\t0\t0.296572\t0.0469684\t0.116667\t0\t1\n",
            "1\t0.0588235\t0.68\t0.606557\t0.5\t0.241135\t0.557377\t0.137062\t0.05\t0\t1\n",
            "1\t0.294118\t0.44\t0.639344\t0.3\t0\t0.411326\t0.0768574\t0.266667\t0\t1\n",
            "1\t0.352941\t0.4\t0.540984\t0.3\t0\t0.390462\t0.100342\t0.333333\t0\t1\n",
            "1\t0.176471\t0.71\t0.655738\t0.15\t0\t0.482861\t0.0520922\t0.7\t0\t1\n",
            "1\t0\t0.5\t0.721311\t0.6\t0.130024\t0.697466\t0.377455\t0.166667\t0\t1\n",
            "1\t0.352941\t0.77\t0.639344\t0.41\t0.165485\t0.687034\t0.210504\t0.1\t0\t1\n",
            "1\t0.0588235\t0.545\t0.311475\t0.18\t0.141844\t0.344262\t0.140478\t0.0833333\t0\t1\n",
            "1\t0.352941\t0.555\t0.52459\t0.39\t0\t0.509687\t0.0777114\t0.05\t0\t1\n",
            "1\t0.352941\t0.915\t0.770492\t0\t0\t0.608048\t0.590521\t0.4\t0\t1\n",
            "1\t0\t0.52\t0.52459\t0.23\t0.137116\t0.414307\t0.160547\t0.0333333\t0\t1\n",
            "1\t0.117647\t0.415\t0.540984\t0.23\t0.0591017\t0.479881\t0.178907\t0.0166667\t0\t1\n",
            "1\t0\t0.485\t0.52459\t0.36\t0.118203\t0.548435\t0.222886\t0.0666667\t0\t1\n",
            "1\t0\t0.595\t0.52459\t0.18\t0.108747\t0.520119\t0.27626\t0.0333333\t0\t1\n",
            "1\t0\t0.53\t0.57377\t0.37\t0.174941\t0.587183\t0.225021\t0.0166667\t0\t1\n",
            "1\t0\t0.99\t0.540984\t0.32\t0.323877\t0.615499\t0.181042\t0.116667\t1\t0\n",
            "1\t0.647059\t0.675\t0\t0\t0\t0.779434\t0.213493\t0.316667\t1\t0\n",
            "1\t0.294118\t0.525\t0.590164\t0.29\t0.384161\t0.549925\t0.0345858\t0.116667\t0\t1\n",
            "1\t0\t0.615\t0.590164\t0\t0\t0.540984\t0.0768574\t0.516667\t1\t0\n",
            "1\t0.235294\t0.475\t0.52459\t0\t0\t0.4769\t0.0354398\t0.166667\t1\t0\n",
            "1\t0.235294\t0.615\t0.508197\t0\t0\t0.4769\t0.0631939\t0.233333\t1\t0\n",
            "1\t0.0588235\t0.475\t0.491803\t0.18\t0.0685579\t0.356185\t0.0777114\t0.0166667\t0\t1\n",
            "1\t0.470588\t0.54\t0.57377\t0\t0\t0.454545\t0.374466\t0.2\t1\t0\n",
            "1\t0.0588235\t0.64\t0.672131\t0.17\t0.216312\t0.409836\t0.0157985\t0.0166667\t0\t1\n",
            "1\t0.411765\t0.68\t0.737705\t0\t0\t0.445604\t0.0563621\t0.483333\t0\t1\n",
            "1\t0.764706\t0.76\t0.737705\t0.33\t0.034279\t0.399404\t0.278822\t0.366667\t1\t0\n",
            "1\t0.235294\t0.585\t0.52459\t0.27\t0.141844\t0.494784\t0.0649018\t0.05\t0\t1\n",
            "1\t0.117647\t0.445\t0.737705\t0.3\t0\t0.499255\t0.0913749\t0.35\t0\t1\n",
            "1\t0.0588235\t0\t0.606557\t0.2\t0.0271868\t0.412817\t0.0943638\t0\t0\t1\n",
            "1\t0.0588235\t0.755\t0.491803\t0\t0\t0.388972\t0.0431255\t0.0166667\t0\t1\n",
            "1\t0.235294\t0.555\t0.590164\t0.47\t0.244681\t0.552906\t0.560205\t0.583333\t1\t0\n",
            "1\t0.176471\t0.87\t0.47541\t0.22\t0.229314\t0.490313\t0.219898\t0.25\t1\t0\n",
            "1\t0.470588\t0.895\t0.590164\t0.42\t0.153664\t0.487332\t0.273698\t0.25\t1\t0\n",
            "1\t0.235294\t0.705\t0.606557\t0\t0\t0.411326\t0.0708796\t0.316667\t0\t1\n",
            "1\t0.705882\t0.46\t0.508197\t0.07\t0.304965\t0.411326\t0.362084\t0.383333\t1\t0\n",
            "1\t0\t0.365\t0\t0\t0\t0.314456\t0.112724\t0.0666667\t0\t1\n",
            "1\t0.588235\t0.645\t0.508197\t0.36\t0\t0.614009\t0.154996\t0.283333\t1\t0\n",
            "1\t0.352941\t0.81\t0.508197\t0\t0\t0.362146\t0.0426985\t0.483333\t1\t0\n",
            "1\t0.117647\t0.53\t0.52459\t0.35\t0.140662\t0.454545\t0.564475\t0.216667\t0\t1\n",
            "1\t0.117647\t0.645\t0.688525\t0\t0\t0.417288\t0.087959\t0.1\t0\t1\n",
            "1\t0\t0.705\t0.688525\t0.26\t0\t0.482861\t0.15158\t0.0166667\t0\t1\n",
            "1\t0.117647\t0.49\t0.491803\t0.17\t0.141844\t0.517139\t0.0512383\t0.0166667\t0\t1\n",
            "1\t0\t0.54\t0.557377\t0.2\t0\t0.406855\t0.302733\t0.183333\t0\t1\n",
            "1\t0.117647\t0.56\t0.557377\t0.22\t0.111111\t0.508197\t0.101196\t0.0833333\t0\t1\n",
            "1\t0.411765\t0.485\t0.622951\t0.32\t0.107565\t0.609538\t0.338599\t0.183333\t1\t0\n",
            "1\t0\t0.76\t0.672131\t0.39\t0.321513\t0.61848\t0.0819812\t0.1\t0\t1\n",
            "1\t0.117647\t0.73\t0\t0\t0\t0.409836\t0.0691716\t0.116667\t1\t0\n",
            "1\t0.0588235\t0.57\t0.540984\t0.36\t0.236407\t0.567809\t0.0900939\t0\t0\t1\n",
            "1\t0.470588\t0.5\t0.606557\t0.4\t0.254137\t0.587183\t0.248933\t0.366667\t1\t0\n",
            "1\t0\t0.725\t0\t0\t0\t0.658718\t0.235696\t0.166667\t1\t0\n",
            "1\t0.647059\t0.6\t0.655738\t0.37\t0.177305\t0.630402\t0.301879\t0.45\t1\t0\n",
            "1\t0.235294\t0.42\t0.737705\t0.23\t0.0661939\t0.588674\t0.0345858\t0.0666667\t0\t1\n",
            "1\t0.470588\t0.97\t0.655738\t0\t0\t0.388972\t0.201964\t0.766667\t0\t1\n",
            "1\t0.235294\t0.685\t0.688525\t0\t0\t0.464978\t0.0742955\t0.15\t0\t1\n",
            "1\t0.470588\t0.55\t0.622951\t0\t0\t0.414307\t0.0678907\t0.616667\t0\t1\n",
            "1\t0.470588\t0.755\t0.639344\t0.32\t0.248227\t0.639344\t0.18702\t0.25\t1\t0\n",
            "1\t0.647059\t0.775\t0.622951\t0.28\t0.177305\t0.496274\t0.544406\t0.5\t1\t0\n",
            "1\t0.0588235\t0.7\t0.606557\t0.26\t0.212766\t0.359165\t0.320239\t0.0333333\t0\t1\n",
            "1\t0.294118\t0.515\t0.885246\t0.37\t0\t0.584203\t0.0969257\t0.733333\t0\t1\n",
            "1\t0.411765\t0.525\t0\t0\t0\t0\t0.0969257\t0.05\t0\t1\n",
            "1\t0.117647\t0.61\t0.42623\t0.43\t0.186761\t0.539493\t0.315115\t0.116667\t0\t1\n",
            "1\t0.588235\t0.895\t0.57377\t0\t0\t0.5231\t0.0520922\t0.266667\t0\t1\n",
            "1\t0.411765\t0.76\t0.721311\t0.44\t0\t0.745156\t0.110589\t0.25\t1\t0\n",
            "1\t0.235294\t0.72\t0.47541\t0.28\t0.165485\t0.439642\t0.08924\t0.266667\t0\t1\n",
            "1\t0.352941\t0.625\t0.557377\t0.3\t0.141844\t0.447094\t0.164816\t0.183333\t0\t1\n",
            "1\t0.0588235\t0.9\t0\t0\t0\t0.645306\t0.087105\t0.333333\t1\t0\n",
            "1\t0.235294\t0.425\t0.47541\t0.22\t0.0579196\t0.414307\t0.0973527\t0.116667\t0\t1\n",
            "1\t0\t0.62\t0.459016\t0.13\t0.124113\t0.324888\t0.159693\t0\t0\t1\n",
            "1\t0.235294\t0.78\t0.614754\t0\t0\t0.719821\t0.0683177\t0.183333\t1\t0\n",
            "1\t0.294118\t0.65\t0.672131\t0\t0\t0.582712\t0.374893\t0.266667\t1\t0\n",
            "1\t0.0588235\t0.445\t0.622951\t0.34\t0.0437352\t0.464978\t0.0486763\t0.0333333\t0\t1\n",
            "1\t0.0588235\t0.61\t0.737705\t0.51\t0.260047\t0.740686\t0.105465\t0.166667\t1\t0\n",
            "1\t0.176471\t0.955\t0.557377\t0.15\t0.153664\t0.460507\t0.0943638\t0.216667\t0\t1\n",
            "1\t0.117647\t0.73\t0.57377\t0.38\t0.425532\t0.417288\t0.110589\t0.133333\t1\t0\n",
            "1\t0.823529\t0.5\t0.639344\t0.25\t0.217494\t0.545455\t0.142613\t0.416667\t1\t0\n",
            "1\t0.117647\t0.5\t0.540984\t0.2\t0.106383\t0.490313\t0.336892\t0.116667\t1\t0\n",
            "1\t0.117647\t0.6\t0.442623\t0\t0\t0.399404\t0.160974\t0.1\t0\t1\n",
            "1\t0.0588235\t0.64\t0.803279\t0.41\t0.0685579\t0.4769\t0.530743\t0.2\t1\t0\n",
            "1\t0.0588235\t0.395\t0.655738\t0.25\t0.0437352\t0.378539\t0.215628\t0.0166667\t0\t1\n",
            "1\t0.0588235\t0.495\t0.47541\t0.1\t0\t0.378539\t0.201964\t0\t0\t1\n",
            "1\t0.294118\t0.945\t0.52459\t0.33\t0.384161\t0.464978\t0.215628\t0.133333\t1\t0\n",
            "1\t0.588235\t0.34\t0.868852\t0.23\t0.0579196\t0.529061\t0.088386\t0.433333\t0\t1\n",
            "1\t0.235294\t0.68\t0.57377\t0\t0\t0.464978\t0.471392\t0.0166667\t1\t0\n",
            "1\t0.235294\t0.495\t0.622951\t0.15\t0.0602837\t0.345753\t0.0619129\t0\t0\t1\n",
            "1\t0.588235\t0.555\t0.57377\t0.27\t0\t0.409836\t0.0269001\t0.316667\t1\t0\n",
            "1\t0\t0.94\t0.672131\t0.14\t0.218676\t0.4769\t0.257899\t0.0166667\t1\t0\n",
            "1\t0.117647\t0.71\t0.672131\t0.18\t0.0756501\t0.368107\t0.291631\t0\t0\t1\n",
            "1\t0.529412\t0.56\t0.672131\t0.32\t0.206856\t0.509687\t0.0777114\t0.25\t1\t0\n",
            "1\t0.470588\t0.775\t0.508197\t0.26\t0.585106\t0.506706\t0.198548\t0.416667\t1\t0\n",
            "1\t0.0588235\t0.715\t0.688525\t0.23\t0.36643\t0.631893\t0.426132\t0.0166667\t0\t1\n",
            "1\t0.588235\t0.81\t0.688525\t0\t0\t0.412817\t0.0444065\t0.55\t0\t1\n",
            "1\t0.470588\t0.98\t0.622951\t0.29\t0.330969\t0.558867\t0.225021\t0.6\t1\t0\n",
            "1\t0.176471\t0.54\t0.508197\t0.24\t0\t0.387481\t0.0619129\t0.0666667\t0\t1\n",
            "1\t0.235294\t0.855\t0.590164\t0\t0\t0.649776\t0.171221\t0.0833333\t1\t0\n",
            "1\t0.294118\t0.715\t0.639344\t0\t0\t0.670641\t0.0478224\t0.433333\t0\t1\n",
            "1\t0.352941\t0.535\t0.721311\t0\t0\t0.548435\t0.277114\t0.166667\t0\t1\n",
            "1\t0.470588\t0.495\t0.688525\t0\t0\t0.527571\t0.132365\t0.483333\t0\t1\n",
            "1\t0.294118\t0.68\t0.688525\t0.41\t0.104019\t0.52161\t0.088813\t0.233333\t1\t0\n",
            "1\t0.0588235\t0.54\t0.721311\t0.19\t0\t0.403875\t0.137489\t0.05\t0\t1\n",
            "1\t0.176471\t0.42\t0.590164\t0.32\t0\t0.554396\t0.0807003\t0.116667\t0\t1\n",
            "1\t0.235294\t0.635\t0.721311\t0.11\t0.183215\t0.514158\t0.222032\t0.116667\t0\t1\n",
            "1\t0\t0.585\t0.655738\t0.31\t0.0626478\t0.673621\t0.00469684\t0.05\t0\t1\n",
            "1\t0\t0.945\t0.852459\t0.25\t0\t0.511177\t0.152434\t0.333333\t1\t0\n",
            "1\t0.176471\t0.415\t0.47541\t0.31\t0.0212766\t0.511177\t0.110162\t0.0666667\t0\t1\n",
            "1\t0.647059\t0.69\t0.606557\t0.26\t0.170213\t0.538003\t0.204526\t0.483333\t1\t0\n",
            "1\t0.470588\t0.42\t0.606557\t0.31\t0\t0.57079\t0.161827\t0.3\t0\t1\n",
            "1\t0.411765\t0.665\t0.688525\t0\t0\t0.599106\t0.263877\t0.266667\t0\t1\n",
            "1\t0\t0.51\t0.704918\t0.17\t0.124113\t0.436662\t0.26345\t0.1\t0\t1\n",
            "1\t0.411765\t0.57\t0.622951\t0.17\t0.130024\t0.354694\t0.16567\t0.166667\t0\t1\n",
            "1\t0.0588235\t0.65\t0.57377\t0.13\t0.124113\t0.385991\t0.168232\t0.0166667\t0\t1\n",
            "1\t0.235294\t0.59\t0.57377\t0\t0\t0.663189\t0.35269\t0.0833333\t0\t1\n",
            "1\t0.823529\t0.875\t0.508197\t0.3\t0\t0.500745\t0.0572161\t0.283333\t1\t0\n",
            "1\t0.117647\t0.475\t0.442623\t0.14\t0.104019\t0.388972\t0.28608\t0.0166667\t0\t1\n",
            "1\t0.0588235\t0.44\t0.508197\t0.24\t0.0520095\t0.445604\t0.146883\t0.0333333\t0\t1\n",
            "1\t0.470588\t0.835\t0.868852\t0.46\t0.27305\t0.560358\t0.0371477\t0.366667\t1\t0\n",
            "1\t0.176471\t0.39\t0.57377\t0\t0\t0.484352\t0.0819812\t0.3\t0\t1\n",
            "1\t0\t0.585\t0.540984\t0.31\t0.222222\t0.459016\t0.177199\t0.0166667\t0\t1\n",
            "1\t0.411765\t0.535\t0.606557\t0\t0\t0.441133\t0.0751494\t0.166667\t1\t0\n",
            "1\t0.294118\t0.695\t0.52459\t0.35\t0.165485\t0.42623\t0.142186\t0.0833333\t0\t1\n",
            "1\t0.117647\t0.45\t0.655738\t0.14\t0.0650118\t0.363636\t0.0730145\t0.05\t0\t1\n",
            "1\t0.117647\t0.72\t0.47541\t0.33\t0.159574\t0.470939\t0.146883\t0.0666667\t1\t0\n",
            "1\t0.529412\t0.65\t0.57377\t0\t0\t0.509687\t0.24509\t0.4\t1\t0\n",
            "1\t0.0588235\t0.98\t0.622951\t0.36\t0.294326\t0.543964\t0.340307\t0.133333\t1\t0\n",
            "1\t0.411765\t0.92\t0.688525\t0.33\t0\t0.529061\t0.118275\t0.333333\t1\t0\n",
            "1\t0.294118\t0.615\t0.606557\t0.4\t0.0910165\t0.508197\t0.0815542\t0.116667\t0\t1\n",
            "1\t0.352941\t0.465\t0.409836\t0.3\t0.0756501\t0.42772\t0.118702\t0.0333333\t0\t1\n",
            "1\t0\t0.735\t0.696721\t0.54\t0\t0.637854\t0.126815\t0.05\t0\t1\n",
            "1\t0.352941\t0.67\t0.655738\t0.37\t0.437352\t0.688525\t0.0683177\t0.416667\t1\t0\n",
            "1\t0.705882\t0.5\t0.688525\t0.33\t0.124113\t0.447094\t0.175064\t0.416667\t0\t1\n",
            "1\t0.411765\t0.62\t0.57377\t0.33\t0.254137\t0.38003\t0.0354398\t0.266667\t0\t1\n",
            "1\t0.705882\t0.44\t0.606557\t0.4\t0.0638298\t0.52608\t0.128096\t0.45\t0\t1\n",
            "1\t0.647059\t0.425\t0.606557\t0\t0\t0.448584\t0.0947908\t0.233333\t0\t1\n",
            "1\t0.588235\t0.45\t0.696721\t0.32\t0\t0.520119\t0.318958\t0.583333\t1\t0\n",
            "1\t0\t0.685\t0.557377\t0.14\t0.174941\t0.369598\t0.0277541\t0\t0\t1\n",
            "1\t0.117647\t0.785\t0.606557\t0.35\t0.520095\t0.587183\t0.0239112\t0.15\t0\t1\n",
            "1\t0.117647\t0.635\t0.47541\t0.24\t0.325059\t0.412817\t0.649872\t0.0666667\t0\t1\n",
            "1\t0\t0.43\t0.557377\t0.32\t0\t0.533532\t0.0683177\t0.0666667\t0\t1\n",
            "1\t0.0588235\t0.785\t0.590164\t0.21\t0.198582\t0.38152\t0.0192143\t0.05\t0\t1\n",
            "1\t0.117647\t0.585\t0.737705\t0.19\t0.0839243\t0.375559\t0.100342\t0\t0\t1\n",
            "1\t0.529412\t0.56\t0.672131\t0.24\t0\t0.420268\t0.514091\t0.483333\t1\t0\n",
            "1\t0.0588235\t0.475\t0.606557\t0.21\t0.0862884\t0.385991\t0.254056\t0.25\t0\t1\n",
            "1\t0.470588\t0.77\t0.639344\t0.32\t0\t0.482861\t0.15585\t0.4\t1\t0\n",
            "1\t0.0588235\t0.56\t0.590164\t0.3\t0.208038\t0.512668\t0.192143\t0.0666667\t0\t1\n",
            "1\t0.411765\t0.735\t0.622951\t0\t0\t0.587183\t0.0764304\t0.366667\t1\t0\n",
            "1\t0.470588\t0.665\t0.590164\t0\t0\t0.490313\t0.0819812\t0.3\t1\t0\n",
            "1\t0.470588\t0.535\t0.655738\t0\t0\t0.366617\t0.332195\t0.216667\t0\t1\n",
            "1\t0.294118\t0.52\t0.606557\t0\t0\t0.42921\t0.0320239\t0.45\t0\t1\n",
            "1\t0.0588235\t0.415\t0.557377\t0\t0\t0.271237\t0.233134\t0.1\t0\t1\n",
            "1\t0.411765\t0.895\t0.778689\t0.31\t0\t0.509687\t0.0367208\t0.65\t0\t1\n",
            "1\t0.0588235\t0.5\t0.540984\t0.15\t0.0661939\t0.351714\t0.251067\t0.0833333\t0\t1\n",
            "1\t0.529412\t0.615\t0.57377\t0.44\t0.111111\t0.493294\t0.126388\t0.316667\t0\t1\n",
            "1\t0.352941\t0.685\t0.5\t0\t0\t0.360656\t0.0311699\t0.566667\t0\t1\n",
            "1\t0.0588235\t0.515\t0.245902\t0.38\t0.0981087\t0.645306\t0.0448335\t0.2\t0\t1\n",
            "1\t0.764706\t0.53\t0.57377\t0\t0\t0.509687\t0.0738685\t0.516667\t0\t1\n",
            "1\t0.294118\t0.735\t0.614754\t0\t0\t0.445604\t0.152007\t0.116667\t0\t1\n",
            "1\t0.235294\t0.495\t0.590164\t0.17\t0\t0.38152\t0.0922289\t0.116667\t0\t1\n",
            "1\t0\t0.57\t0.655738\t0.34\t0.336879\t0.658718\t0.0380017\t0.1\t0\t1\n",
            "1\t0\t0.81\t0.622951\t0.36\t0\t0.739195\t0.122118\t0.0833333\t1\t0\n",
            "1\t0.176471\t0.555\t0.508197\t0\t0\t0.336811\t0.0273271\t0\t0\t1\n",
            "1\t0\t0.5\t0.57377\t0.26\t0.0591017\t0.459016\t0.221605\t0\t0\t1\n",
            "1\t0.294118\t0.79\t0.57377\t0\t0\t0.444113\t0.0550811\t0.7\t0\t1\n",
            "1\t0\t0.505\t0.622951\t0\t0\t0.532042\t0.0512383\t0.0833333\t0\t1\n",
            "1\t0.235294\t0.66\t0.704918\t0.31\t0\t0.417288\t0.145602\t0.7\t0\t1\n",
            "1\t0.0588235\t0.515\t0.655738\t0.11\t0.0969267\t0.289121\t0.176345\t0.0166667\t0\t1\n",
            "1\t0.0588235\t0.44\t0.245902\t0.42\t0.117021\t0.819672\t0.17848\t0.0833333\t1\t0\n",
            "1\t0\t0.645\t0.901639\t0.46\t0.153664\t1\t0.102904\t0.0833333\t1\t0\n",
            "1\t0.235294\t0.945\t0.901639\t0.31\t0\t0.424739\t0.257045\t0.266667\t0\t1\n",
            "1\t0.411765\t0.515\t0.540984\t0.32\t0\t0.582712\t0.113578\t0.166667\t1\t0\n",
            "1\t0.117647\t0.73\t0.622951\t0.35\t0.229314\t0.5693\t0.107173\t0.133333\t0\t1\n",
            "1\t0\t0.625\t0.557377\t0\t0\t0.368107\t0.0546541\t0\t0\t1\n",
            "1\t0.176471\t0.4\t0.672131\t0.31\t0.0827423\t0.509687\t0.51836\t0.1\t1\t0\n",
            "1\t0.0588235\t0.765\t0.672131\t0.42\t0.573286\t0.605067\t0.260034\t0.0333333\t0\t1\n",
            "1\t0.0588235\t0.64\t0.393443\t0.45\t0.229314\t0.603577\t0.228437\t0.05\t1\t0\n",
            "1\t0.0588235\t0.475\t0.540984\t0.13\t0.0449173\t0.292101\t0.109308\t0.0666667\t0\t1\n",
            "1\t0.647059\t0.715\t0.770492\t0.33\t0.172577\t0.545455\t0.0751494\t0.5\t1\t0\n",
            "1\t0.588235\t0.74\t0.688525\t0.48\t0.280142\t0.560358\t0.394108\t0.5\t1\t0\n",
            "1\t0.411765\t0.89\t0.688525\t0\t0\t0.594635\t0.108027\t0.333333\t1\t0\n",
            "1\t0\t0.585\t0\t0\t0\t0.503726\t0.364646\t0.383333\t0\t1\n",
            "1\t0.176471\t0.615\t0.819672\t0.35\t0.283688\t0.853949\t0.342442\t0.0166667\t0\t1\n",
            "1\t0.352941\t0.455\t0\t0\t0\t0.444113\t0.180615\t0.166667\t0\t1\n",
            "1\t0.352941\t0.735\t0.655738\t0\t0\t0.439642\t0.0426985\t0.483333\t1\t0\n",
            "1\t0.117647\t0.415\t0.532787\t0.28\t0.0780142\t0.548435\t0.235269\t0.05\t0\t1\n",
            "1\t0.588235\t0.695\t0.655738\t0\t0\t0.403875\t0.581981\t0.6\t0\t1\n",
            "1\t0.117647\t0.45\t0.557377\t0.42\t0\t0.5693\t0.181469\t0.1\t1\t0\n",
            "1\t0.176471\t0.75\t0.622951\t0\t0\t0.312966\t0.0550811\t0.266667\t0\t1\n",
            "1\t0\t0.885\t0.491803\t0.29\t0.565012\t0.515648\t0.424424\t0\t1\t0\n",
            "1\t0.470588\t0.63\t0.721311\t0.36\t0.12766\t0.57377\t0.115713\t0.466667\t0\t1\n",
            "1\t0.235294\t0.92\t0.639344\t0.39\t0.327423\t0.551416\t0.0794193\t0.166667\t1\t0\n",
            "1\t0.0588235\t0.61\t0.52459\t0.32\t0.184397\t0.5231\t0.262169\t0.15\t1\t0\n",
            "1\t0.411765\t0.795\t0.52459\t0\t0\t0.408346\t0.0922289\t0.316667\t0\t1\n",
            "1\t0.0588235\t0.5\t0.590164\t0.12\t0.0827423\t0.377049\t0.247652\t0.116667\t0\t1\n",
            "1\t0.176471\t0.74\t0.540984\t0.25\t0\t0.484352\t0.0760034\t0.0166667\t0\t1\n",
            "1\t0.294118\t0.575\t0.803279\t0\t0\t0.788376\t0.0559351\t0.116667\t1\t0\n",
            "1\t0.117647\t0.51\t0.704918\t0.36\t0.141844\t0.678092\t0.0209223\t0.0333333\t1\t0\n",
            "1\t0.176471\t0.61\t0.639344\t0\t0\t0.342772\t0.0751494\t0.316667\t0\t1\n",
            "1\t0\t0.51\t0.52459\t0.46\t0.0921986\t0.605067\t0.17848\t0\t0\t1\n",
            "1\t0.117647\t0.62\t0.557377\t0.28\t0.242317\t0.490313\t0.340307\t0.15\t1\t0\n",
            "1\t0.529412\t0.7\t0.770492\t0\t0\t0.487332\t0.280102\t0.4\t1\t0\n",
            "1\t0.0588235\t0.665\t0.836066\t0.28\t0.165485\t0.488823\t0.0666097\t0.4\t1\t0\n",
            "1\t0.0588235\t0.45\t0.508197\t0.18\t0.06974\t0.374069\t0.508113\t0.0666667\t0\t1\n",
            "1\t0.294118\t0.72\t0.672131\t0.26\t0.336879\t0.4769\t0.159693\t0.616667\t1\t0\n",
            "1\t0.588235\t0.575\t0.803279\t0\t0\t0.357675\t0.403074\t0.216667\t0\t1\n",
            "1\t0.117647\t0.64\t0.639344\t0.37\t0.21513\t0.645306\t0.489325\t0.166667\t1\t0\n",
            "1\t0.117647\t0.56\t0.704918\t0.42\t0.189125\t0.57228\t0.0717336\t0.116667\t0\t1\n",
            "1\t0\t0.64\t0.557377\t0.19\t0.212766\t0.454545\t0.560632\t0.0666667\t1\t0\n",
            "1\t0.529412\t0.53\t0.42623\t0\t0\t0.464978\t0.12895\t0.35\t0\t1\n",
            "1\t0.117647\t0.605\t0.57377\t0.32\t0.112293\t0.582712\t0.345004\t0.0333333\t0\t1\n",
            "1\t0.529412\t0.82\t0.688525\t0.21\t0\t0.459016\t0.32152\t0.183333\t1\t0\n",
            "1\t0.117647\t0.575\t0.52459\t0.22\t0\t0.459016\t0.146456\t0\t0\t1\n",
            "1\t0.0588235\t0.695\t0.508197\t0.41\t0.567376\t0.606557\t0.195559\t0\t0\t1\n",
            "1\t0.117647\t0.775\t0.42623\t0.27\t0.638298\t0.576751\t0.0691716\t0.0666667\t1\t0\n",
            "1\t0.176471\t0.555\t0.737705\t0.12\t0.0921986\t0.423249\t0.178053\t0.133333\t0\t1\n",
            "1\t0.235294\t0.755\t0.737705\t0.38\t0\t0.442623\t0.0922289\t0.25\t0\t1\n",
            "1\t0.0588235\t0.595\t0.721311\t0.41\t0.200946\t0.675112\t0.183177\t0.0833333\t0\t1\n",
            "1\t0.0588235\t0.63\t0.459016\t0.29\t0.179669\t0.42772\t0.308711\t0\t0\t1\n",
            "1\t0.117647\t0.44\t0.47541\t0.26\t0.0189125\t0.423249\t0.293766\t0.0166667\t0\t1\n",
            "1\t0\t0.695\t0.508197\t0.17\t0.248227\t0.329359\t0.0550811\t0\t0\t1\n",
            "1\t0.176471\t0.53\t0.590164\t0\t0\t0.384501\t0.0550811\t0.1\t0\t1\n",
            "1\t0.117647\t0.495\t0\t0\t0\t0.330849\t0.0128096\t0.0333333\t0\t1\n",
            "1\t0.294118\t0.605\t0.590164\t0.23\t0.132388\t0.390462\t0.0713066\t0.15\t0\t1\n",
            "1\t0.0588235\t0.59\t0.47541\t0.36\t0.111111\t0.496274\t0.0781383\t0.0333333\t0\t1\n",
            "1\t0.235294\t0.45\t0.721311\t0.47\t0.0638298\t0.561848\t0.121264\t0.133333\t0\t1\n",
            "1\t0\t0.895\t0.409836\t0.36\t0.187943\t0.563338\t0.160974\t0.0166667\t1\t0\n",
            "1\t0.0588235\t0.595\t0.704918\t0.39\t0.260047\t0.679583\t0.311699\t0.133333\t1\t0\n",
            "1\t0.764706\t0.765\t0.721311\t0.37\t0.165485\t0.605067\t0.467976\t0.3\t0\t1\n",
            "1\t0.176471\t0.9\t0.52459\t0.25\t0.0827423\t0.506706\t0.0824082\t0.0833333\t0\t1\n",
            "1\t0.470588\t0.59\t0.590164\t0.19\t0\t0.344262\t0.596926\t0.416667\t0\t1\n",
            "1\t0.0588235\t0.62\t0.606557\t0.36\t0\t0.414307\t0.00939368\t0.15\t0\t1\n",
            "1\t0.0588235\t0.905\t0.52459\t0.3\t0.212766\t0.508197\t0.106746\t0.283333\t1\t0\n",
            "1\t0.294118\t0.585\t0.704918\t0.3\t0.124113\t0.582712\t0.0738685\t0.35\t0\t1\n",
            "1\t0.117647\t0.775\t0.606557\t0.17\t0.113475\t0.396423\t0.15158\t0.1\t1\t0\n",
            "1\t0.117647\t0.54\t0.52459\t0\t0\t0.459016\t0.0341588\t0\t0\t1\n",
            "1\t0.117647\t0.54\t0.42623\t0.26\t0.0744681\t0.484352\t0.102477\t0.0166667\t0\t1\n",
            "1\t0.0588235\t0.585\t0.721311\t0.24\t0.171395\t0.514158\t0.13877\t0.316667\t1\t0\n",
            "1\t0.470588\t0.94\t0.639344\t0\t0\t0.71386\t0.0251921\t0.366667\t1\t0\n",
            "1\t0.235294\t0.495\t0.557377\t0.38\t0\t0.488823\t0.028608\t0.2\t0\t1\n",
            "1\t0.0588235\t0.535\t0.409836\t0.19\t0\t0.421759\t0.0439795\t0.133333\t0\t1\n",
            "1\t0\t0.505\t0.532787\t0.28\t0\t0.366617\t0.0678907\t0.0166667\t0\t1\n",
            "1\t0.176471\t0.51\t0.360656\t0.2\t0.111111\t0.459016\t0.137489\t0.0833333\t0\t1\n",
            "1\t0.352941\t0.575\t0.491803\t0.39\t0\t0.502235\t0.0713066\t0.316667\t1\t0\n",
            "1\t0.0588235\t0.6\t0.655738\t0.48\t0.236407\t0.579732\t0.462852\t0.333333\t0\t1\n",
            "1\t0.294118\t0.585\t0.754098\t0\t0\t0.508197\t0.110589\t0.283333\t0\t1\n",
            "1\t0.647059\t0.68\t0.688525\t0.35\t0.153664\t0.421759\t0.0777114\t0.35\t1\t0\n",
            "1\t0.0588235\t0.58\t0.57377\t0.28\t0\t0.408346\t0.0538002\t0\t0\t1\n",
            "1\t0.588235\t0.47\t0.590164\t0.18\t0\t0.344262\t0.220751\t0.583333\t0\t1\n",
            "1\t0.176471\t0.79\t0.57377\t0.3\t0.387707\t0.529061\t0.113578\t0.233333\t1\t0\n",
            "1\t0.529412\t0.825\t0.721311\t0\t0\t0.453055\t0.0956447\t0.466667\t1\t0\n",
            "1\t0.117647\t0.405\t0.590164\t0.15\t0.0898345\t0.448584\t0.200256\t0.0666667\t0\t1\n",
            "1\t0.0588235\t0.355\t0.508197\t0\t0\t0.324888\t0.144321\t0.0833333\t0\t1\n",
            "1\t0.117647\t0.46\t0.42623\t0\t0\t0.448584\t0.0269001\t0.0166667\t0\t1\n",
            "1\t0\t0.73\t0.672131\t0\t0\t0.603577\t0.727156\t0.383333\t0\t1\n",
            "1\t0.0588235\t0.405\t0.590164\t0.18\t0.0472813\t0.396423\t0.087532\t0.05\t0\t1\n",
            "1\t0.176471\t0.575\t0.540984\t0.39\t0.165485\t0.567809\t0.030743\t0.116667\t0\t1\n",
            "1\t0.117647\t0.625\t0.491803\t0.2\t0.165485\t0.503726\t0.00426985\t0.166667\t0\t1\n",
            "1\t0.176471\t0.555\t0.459016\t0.39\t0\t0.448584\t0.204526\t0.15\t0\t1\n",
            "1\t0.176471\t0.65\t0.639344\t0.23\t0.0933806\t0.423249\t0.104611\t0.216667\t1\t0\n",
            "1\t0.294118\t0.425\t0.606557\t0.22\t0\t0.432191\t0.489325\t0.183333\t1\t0\n",
            "1\t0.117647\t0.5\t0.442623\t0.28\t0.124113\t0.563338\t0.179334\t0.05\t0\t1\n",
            "1\t0.235294\t0.6\t0.557377\t0\t0\t0.441133\t0.269428\t0.216667\t0\t1\n",
            "1\t0.0588235\t0.53\t0.622951\t0\t0\t0.558867\t0.0508113\t0.0833333\t0\t1\n",
            "1\t0.117647\t0.5\t0.52459\t0.23\t0\t0.442623\t0.123826\t0\t0\t1\n",
            "1\t0.352941\t0.585\t0.786885\t0\t0\t0.42772\t0.0337319\t0.15\t0\t1\n",
            "1\t0.176471\t0.66\t0.655738\t0\t0\t0.512668\t0.138343\t0.383333\t1\t0\n",
            "1\t0.529412\t0.455\t0.557377\t0\t0\t0.360656\t0.0520922\t0.616667\t0\t1\n",
            "1\t0.235294\t0.72\t0.672131\t0.32\t0\t0.57377\t0.203245\t0.266667\t1\t0\n",
            "1\t0\t0.51\t0.614754\t0.23\t0\t0\t0.210931\t0\t0\t1\n",
            "1\t0.235294\t0.79\t0.639344\t0\t0\t0.490313\t0.309564\t0.166667\t1\t0\n",
            "1\t0.352941\t0.83\t0.606557\t0\t0\t0.396423\t0.0964987\t0.75\t0\t1\n",
            "1\t0.0588235\t0.385\t0.459016\t0.3\t0.0661939\t0.496274\t0.500854\t0.05\t0\t1\n",
            "1\t0.0588235\t0.5\t0.540984\t0.29\t0.231678\t0.4769\t0.156277\t0.35\t0\t1\n",
            "1\t0.176471\t0.515\t0.590164\t0.3\t0.179669\t0.411326\t0.278395\t0.1\t0\t1\n",
            "1\t0.176471\t0.405\t0.704918\t0.16\t0.0780142\t0.409836\t0.0973527\t0.0166667\t0\t1\n",
            "1\t0.470588\t0.37\t0.57377\t0.4\t0.0579196\t0.52608\t0.26772\t0.3\t0\t1\n",
            "1\t0.176471\t0.445\t0.606557\t0.16\t0.100473\t0.453055\t0.201964\t0.283333\t0\t1\n",
            "1\t0.352941\t0.515\t0.540984\t0\t0\t0.362146\t0.0730145\t0.133333\t0\t1\n",
            "1\t0.0588235\t0.625\t0.409836\t0.4\t0.1974\t0.496274\t0.377455\t0.116667\t1\t0\n",
            "1\t0.470588\t0.455\t0.672131\t0\t0\t0.530551\t0.217336\t0.783333\t0\t1\n",
            "1\t1\t0.815\t0.590164\t0.41\t0.134752\t0.609538\t0.315542\t0.433333\t1\t0\n",
            "1\t0.294118\t0.53\t0.672131\t0.3\t0\t0.588674\t0.088813\t0.283333\t0\t1\n",
            "1\t0.352941\t0.425\t0.639344\t0\t0\t0.464978\t0.129804\t0.35\t0\t1\n",
            "1\t0.235294\t0.625\t0.57377\t0.18\t0.144208\t0.4307\t0.455167\t0.4\t1\t0\n",
            "1\t0\t0.52\t0.622951\t0\t0\t0.274218\t0.215201\t0.1\t0\t1\n",
            "1\t0.117647\t0.47\t0.622951\t0.18\t0.0780142\t0.470939\t0.243809\t0.0333333\t0\t1\n",
            "1\t0.235294\t0.56\t0.639344\t0.4\t0\t0.587183\t0.0674637\t0.283333\t0\t1\n",
            "1\t0.529412\t0.725\t0.721311\t0.34\t0.195035\t0.451565\t0.295901\t0.533333\t1\t0\n",
            "1\t0.176471\t0.645\t0.52459\t0.29\t0.135934\t0.393443\t0.060205\t0.116667\t1\t0\n",
            "1\t0.117647\t0.375\t0.52459\t0.24\t0.0650118\t0.442623\t0.12468\t0.2\t0\t1\n",
            "1\t0.705882\t0.42\t0.590164\t0.31\t0\t0.442623\t0.0935098\t0.416667\t1\t0\n",
            "1\t0.176471\t0.305\t0.672131\t0.28\t0\t0.512668\t0.0704526\t0.416667\t0\t1\n",
            "1\t0.0588235\t0.455\t0.442623\t0.25\t0.118203\t0.375559\t0.0666097\t0.0333333\t0\t1\n",
            "1\t0.0588235\t0.82\t0.672131\t0.43\t0.0791962\t0.488823\t0.112297\t0.483333\t0\t1\n",
            "1\t0\t0.705\t0\t0\t0\t0.631893\t0.0542272\t0.133333\t1\t0\n",
            "1\t0.0588235\t0.595\t0.360656\t0.47\t0.0744681\t0.529061\t0.0862511\t0.0666667\t0\t1\n",
            "1\t0.0588235\t0.48\t0.52459\t0.27\t0.102837\t0.494784\t0.0900939\t0\t0\t1\n",
            "1\t0.294118\t0.695\t0.655738\t0.35\t0.189125\t0.470939\t0.120837\t0.0666667\t1\t0\n",
            "1\t0.470588\t0.905\t0.557377\t0.36\t0.585106\t0.448584\t0.229291\t0.65\t1\t0\n",
            "1\t0.411765\t0.405\t0.639344\t0.4\t0.0567376\t0.695976\t0.0781383\t0.35\t0\t1\n",
            "1\t0.529412\t0.725\t0.655738\t0.46\t0.153664\t0.564829\t0.238685\t0.316667\t1\t0\n",
            "1\t0\t0.635\t0.655738\t0.37\t0.248227\t0.540984\t0.309991\t0.0333333\t0\t1\n",
            "1\t0.647059\t0.555\t0.688525\t0.4\t0\t0.697466\t0.361657\t0.4\t1\t0\n",
            "1\t0.411765\t0.71\t0.491803\t0.33\t0.224586\t0.42921\t0.260034\t0.666667\t0\t1\n",
            "1\t0.411765\t0.595\t0\t0\t0\t0.375559\t0.0559351\t0.266667\t0\t1\n",
            "1\t0.0588235\t0.715\t0.606557\t0.22\t0.072104\t0.390462\t0.0760034\t0\t0\t1\n",
            "1\t0.0588235\t0.435\t0.557377\t0.34\t0.0910165\t0.560358\t0.137916\t0.05\t0\t1\n",
            "1\t0.411765\t0.415\t0.639344\t0.26\t0.0839243\t0.436662\t0.294193\t0.25\t0\t1\n",
            "1\t0.470588\t0.6\t0.639344\t0\t0\t0.372578\t0.141332\t0.716667\t0\t1\n",
            "1\t0.0588235\t0.445\t0.540984\t0.23\t0.111111\t0.418778\t0.0380017\t0\t0\t1\n",
            "1\t0.470588\t0.915\t0.52459\t0\t0\t0.347243\t0.253629\t0.183333\t1\t0\n",
            "1\t0.352941\t0.625\t0.639344\t0.31\t0\t0.411326\t0.207942\t0.466667\t1\t0\n",
            "1\t0.0588235\t0.355\t0.639344\t0.5\t0.0531915\t0.494784\t0.146883\t0\t0\t1\n",
            "1\t0.176471\t0.63\t0.721311\t0.41\t0.277778\t0.585693\t0.267293\t0.1\t0\t1\n",
            "1\t0.588235\t0.625\t0.57377\t0.26\t0.135934\t0.463487\t0.0542272\t0.333333\t1\t0\n",
            "1\t0\t0.535\t0.491803\t0.25\t0\t0.393443\t0.0234842\t0.0333333\t0\t1\n",
            "1\t0.529412\t0.285\t0.655738\t0.37\t0\t0.488823\t0.00768574\t0.333333\t0\t1\n",
            "1\t0.294118\t0.43\t0.557377\t0.28\t0.0839243\t0.450075\t0.122118\t0.05\t0\t1\n",
            "1\t0.117647\t0.44\t0.606557\t0.19\t0.0626478\t0.432191\t0.0644748\t0.0166667\t0\t1\n",
            "1\t0.176471\t0.565\t0.360656\t0.13\t0\t0.33383\t0.0264731\t0.0166667\t0\t1\n",
            "1\t0.0588235\t0.41\t0.52459\t0.13\t0.112293\t0.315946\t0.143894\t0.0333333\t0\t1\n",
            "1\t0.235294\t0.45\t0\t0\t0\t0.417288\t0.227156\t0.166667\t0\t1\n",
            "1\t0.588235\t0.46\t0.508197\t0\t0\t0.385991\t0.0380017\t0.166667\t0\t1\n",
            "1\t0.235294\t0.77\t0.590164\t0.29\t0.148936\t0.466468\t0.111016\t0.266667\t0\t1\n",
            "1\t0.176471\t0.85\t0.52459\t0.37\t0.265957\t0.514158\t0.118702\t0.15\t1\t0\n",
            "1\t0.764706\t0.79\t0.934426\t0\t0\t0.630402\t0.0764304\t0.383333\t1\t0\n",
            "1\t0.352941\t0.97\t0.639344\t0\t0\t0.350224\t0.0217763\t0.633333\t1\t0\n",
            "1\t0.0588235\t0.575\t0.57377\t0.3\t0.113475\t0.515648\t0.19257\t0.183333\t1\t0\n",
            "1\t0.294118\t0\t0.655738\t0.32\t0\t0.611028\t0.114432\t0.266667\t1\t0\n",
            "1\t0.235294\t0.46\t0.655738\t0\t0\t0.628912\t0.0678907\t0.133333\t0\t1\n",
            "1\t0.411765\t0.805\t0.704918\t0\t0\t0.453055\t0.0371477\t0.433333\t1\t0\n",
            "1\t0.764706\t0.63\t0.737705\t0\t0\t0.646796\t0.215628\t0.35\t1\t0\n",
            "1\t0.176471\t0.695\t0.442623\t0\t0\t0.38152\t0.138343\t0.0166667\t1\t0\n",
            "1\t0.0588235\t0.815\t0.590164\t0\t0\t0.581222\t0.488471\t0.2\t1\t0\n",
            "1\t0.411765\t0.625\t0.704918\t0\t0\t0.560358\t0.0964987\t0.5\t0\t1\n",
            "1\t0\t0.655\t0\t0\t0\t0.643815\t0.0819812\t0.0833333\t1\t0\n",
            "1\t0.470588\t0.93\t0.737705\t0.35\t0.265957\t0.514158\t0.14731\t0.266667\t1\t0\n",
            "1\t0.294118\t0.61\t0.704918\t0\t0\t0.517139\t0.0905209\t0.2\t0\t1\n",
            "1\t0\t0.645\t0.655738\t0\t0\t0.464978\t0.266866\t0.133333\t0\t1\n",
            "1\t0.352941\t0.525\t0.57377\t0.32\t0.0803783\t0.459016\t0.0187874\t0.266667\t0\t1\n",
            "1\t0.235294\t0.64\t0.57377\t0\t0\t0.511177\t0.0960717\t0.05\t0\t1\n",
            "1\t0.529412\t0.76\t0.639344\t0.34\t0.202128\t0.509687\t0.347993\t0.2\t1\t0\n",
            "1\t0.588235\t0.375\t0.672131\t0\t0\t0.496274\t0.0789923\t0.283333\t0\t1\n",
            "1\t0.117647\t0.61\t0.57377\t0.27\t0\t0.548435\t0.11187\t0.1\t0\t1\n",
            "1\t0.352941\t0.51\t0.672131\t0\t0\t0.459016\t0.0435525\t0.25\t1\t0\n",
            "1\t0.117647\t0.615\t0.393443\t0.32\t0.195035\t0.627422\t0.188728\t0.0833333\t0\t1\n",
            "1\t0.0588235\t0.64\t0.721311\t0.39\t0.130024\t0.543964\t0.418019\t0.266667\t1\t0\n",
            "1\t0.176471\t0.81\t0.42623\t0.38\t0\t0.554396\t0.24509\t0.05\t1\t0\n",
            "1\t0.176471\t0.705\t0\t0\t0\t0.447094\t0.291631\t0.1\t1\t0\n",
            "1\t0.117647\t0.705\t0.47541\t0.34\t0.1513\t0.378539\t0.265158\t0.05\t0\t1\n",
            "1\t0\t0.835\t0\t0\t0\t0.481371\t0.324936\t0.15\t1\t0\n",
            "1\t0.176471\t0.865\t0.639344\t0.39\t0.218676\t0.503726\t0.380871\t0.166667\t1\t0\n",
            "1\t0.0588235\t0.545\t0.491803\t0.08\t0.21513\t0.378539\t0.37105\t0\t0\t1\n",
            "1\t0.352941\t0.57\t0.721311\t0\t0\t0.414307\t0.0721605\t0.75\t0\t1\n",
            "1\t0\t0.655\t0.540984\t0.4\t0\t0.511177\t0.0503843\t0.0166667\t1\t0\n",
            "1\t0.0588235\t0.715\t0.704918\t0.3\t0.390071\t0.448584\t0.347566\t0.0333333\t0\t1\n",
            "1\t0.117647\t0.45\t0.491803\t0\t0\t0.350224\t0.0482494\t0.0666667\t0\t1\n",
            "1\t0\t0.535\t0.622951\t0\t0\t0.675112\t0.259607\t0.05\t0\t1\n",
            "1\t0\t0.685\t0.688525\t0.27\t0\t0.406855\t0.0653288\t0.633333\t0\t1\n",
            "1\t0.411765\t0.8\t0.442623\t0.32\t0.206856\t0.454545\t0.217763\t0.3\t1\t0\n",
            "1\t0.117647\t0.505\t0.47541\t0.17\t0.313239\t0.360656\t0.228864\t0.0333333\t0\t1\n",
            "1\t0.235294\t0.625\t0.655738\t0\t0\t0.481371\t0.195559\t0.1\t1\t0\n",
            "1\t0.294118\t0.775\t0.688525\t0.44\t0.644208\t0.576751\t0.230999\t0.216667\t0\t1\n",
            "1\t0\t0.69\t0\t0\t0\t0.540984\t0.365073\t0.0666667\t1\t0\n",
            "1\t0.411765\t0.97\t0.557377\t0.28\t0\t0.535022\t0.284799\t0.333333\t1\t0\n",
            "1\t0.588235\t0.505\t0.622951\t0.48\t0.212766\t0.490313\t0.0397096\t0.7\t0\t1\n",
            "1\t0.176471\t0.48\t0.459016\t0.34\t0.135934\t0.368107\t0.369769\t0.3\t0\t1\n",
            "1\t0\t0.495\t0\t0\t0\t0.372578\t0.0747225\t0.0166667\t0\t1\n",
            "1\t0\t0.675\t0.770492\t0.46\t0.171395\t0.605067\t0.087959\t0.0833333\t0\t1\n",
            "1\t0.0588235\t0.44\t0.639344\t0.29\t0.0898345\t0.4769\t0.122545\t0.133333\t0\t1\n",
            "1\t0.294118\t0.22\t0.508197\t0\t0\t0.372578\t0.217336\t0.25\t0\t1\n",
            "1\t0.117647\t0.435\t0.47541\t0.16\t0.0614657\t0.487332\t0.0375747\t0.0666667\t0\t1\n",
            "1\t0.0588235\t0.945\t0.491803\t0.23\t1\t0.448584\t0.136635\t0.633333\t1\t0\n",
            "1\t0.411765\t0.5\t0\t0\t0\t0.447094\t0.173356\t0.183333\t1\t0\n",
            "1\t0.294118\t0.57\t0.606557\t0\t0\t0.371088\t0.284372\t0.6\t0\t1\n",
            "1\t0.176471\t0.62\t0.655738\t0.33\t0.153664\t0.494784\t0.0969257\t0.0833333\t0\t1\n",
            "1\t0.235294\t0.455\t0.57377\t0.32\t0.104019\t0.493294\t0.157131\t0.0166667\t0\t1\n",
            "1\t0.117647\t0.54\t0.508197\t0.32\t0.0661939\t0.375559\t0.0213493\t0\t0\t1\n",
            "1\t0.0588235\t0.625\t0.57377\t0.24\t0.130024\t0.362146\t0.0610589\t0.0666667\t0\t1\n",
            "1\t0.352941\t0.51\t0.737705\t0.39\t0\t0.532042\t0.254483\t0.116667\t0\t1\n",
            "1\t0.294118\t0.935\t0.622951\t0.27\t0.244681\t0.649776\t0.408198\t0.533333\t1\t0\n",
            "1\t0.117647\t0.495\t0.42623\t0.15\t0.111111\t0.366617\t0.238685\t0\t0\t1\n",
            "1\t0.411765\t0.68\t0.606557\t0.26\t0.159574\t0.387481\t0.242955\t0.5\t0\t1\n",
            "1\t0\t0.51\t0.42623\t0\t0\t0.374069\t0\t0\t0\t1\n",
            "1\t0\t0.7\t0.532787\t0.26\t0.153664\t0.634873\t0.150726\t0.05\t1\t0\n",
            "1\t0\t0.675\t0.557377\t0.42\t0.295508\t0.630402\t0.122545\t0.05\t1\t0\n",
            "1\t0.117647\t0.525\t0.614754\t0\t0\t0.347243\t0.205807\t0.533333\t0\t1\n",
            "1\t0.117647\t0.695\t0.614754\t0\t0\t0.38152\t0.0380017\t0.133333\t0\t1\n",
            "1\t0\t0.475\t0.52459\t0.39\t0.124113\t0.66468\t0.122972\t0.0166667\t0\t1\n",
            "1\t0.352941\t0.525\t0.655738\t0.28\t0\t0.484352\t0.341588\t0.0833333\t0\t1\n",
            "1\t0.117647\t0.53\t0.459016\t0.27\t0.195035\t0.432191\t0.148591\t0.0166667\t0\t1\n",
            "1\t0.117647\t0.985\t0.57377\t0.45\t0.641844\t0.454545\t0.0341588\t0.533333\t1\t0\n",
            "1\t0.588235\t0.84\t0.606557\t0\t0\t0.566319\t0.195986\t0.216667\t1\t0\n",
            "1\t0.0588235\t0.565\t0.52459\t0.35\t0\t0.500745\t0.198548\t0\t1\t0\n",
            "1\t0.411765\t0.935\t0.557377\t0.39\t0.359338\t0.561848\t0.0751494\t0.333333\t1\t0\n",
            "1\t0.764706\t0.645\t0\t0.3\t0\t0.594635\t0.20965\t0.383333\t1\t0\n",
            "1\t0.235294\t0.645\t0.704918\t0.2\t0.319149\t0.5231\t0.0653288\t0.0333333\t0\t1\n",
            "1\t0.117647\t0.47\t0.557377\t0.18\t0.0898345\t0.387481\t0.206234\t0\t0\t1\n",
            "1\t0.235294\t0.55\t0.540984\t0\t0\t0.47541\t0.167805\t0.133333\t0\t1\n",
            "1\t0.117647\t0.48\t0.557377\t0.13\t0.0579196\t0.314456\t0.242955\t0.0833333\t0\t1\n",
            "1\t0\t0.535\t0.508197\t0.3\t0.0874704\t0.545455\t0.289923\t0.0666667\t1\t0\n",
            "1\t0.176471\t0.42\t0.557377\t0.3\t0.125296\t0.47541\t0.219044\t0.0666667\t0\t1\n",
            "1\t0\t0.66\t0.639344\t0\t0\t0.482861\t0.1345\t0\t0\t1\n",
            "1\t0.117647\t0.465\t0.52459\t0.32\t0.189125\t0.566319\t0.254483\t0.0333333\t1\t0\n",
            "1\t0.411765\t0.665\t0.721311\t0.15\t0.183215\t0.482861\t0.0785653\t0.266667\t0\t1\n",
            "1\t0.529412\t0.92\t0.696721\t0.15\t0\t0.447094\t0.484629\t0.466667\t1\t0\n",
            "1\t0.0588235\t0.555\t0.704918\t0.19\t0\t0.448584\t0.0277541\t0.0333333\t0\t1\n",
            "1\t0.470588\t0.625\t0.786885\t0\t0\t0\t0.0657558\t0.55\t1\t0\n",
            "1\t0.0588235\t0.5\t0.606557\t0.12\t0.0543735\t0.290611\t0.030316\t0.116667\t0\t1\n",
            "1\t0\t0.62\t0.57377\t0.2\t0\t0.408346\t0.0751494\t0.25\t1\t0\n",
            "1\t0.235294\t0.475\t0.57377\t0.32\t0\t0.47839\t0.22801\t0.05\t0\t1\n",
            "1\t0.411765\t0.84\t0.721311\t0.42\t0.379433\t0.5693\t0.302733\t0.316667\t1\t0\n",
            "1\t0.352941\t0.545\t0.491803\t0.27\t0\t0.372578\t0.0546541\t0.1\t0\t1\n",
            "1\t0.235294\t0.475\t0.491803\t0.32\t0\t0.527571\t0.087959\t0.116667\t0\t1\n",
            "1\t0.0588235\t0.86\t0.557377\t0.49\t0.684397\t0.631893\t0.266439\t0.116667\t1\t0\n",
            "1\t0\t0.865\t0.639344\t0.32\t0.313239\t0.692996\t0.461571\t0.616667\t0\t1\n",
            "1\t0.294118\t0.485\t0.622951\t0.27\t0\t0.530551\t0.128096\t0.516667\t1\t0\n",
            "1\t0.117647\t0.555\t0.491803\t0\t0\t0.390462\t0.113151\t0.0333333\t0\t1\n",
            "1\t0.0588235\t0.475\t0.672131\t0.25\t0.212766\t0.52161\t0.0661827\t0.366667\t1\t0\n",
            "1\t0.294118\t0.48\t0.606557\t0.18\t0.0791962\t0.500745\t0.3924\t0.366667\t0\t1\n",
            "1\t0.117647\t0.34\t0.57377\t0.32\t0.0780142\t0.372578\t0.0465414\t0.0666667\t0\t1\n",
            "1\t0.0588235\t0.965\t0.409836\t0.16\t0.443262\t0.385991\t0.246371\t0.05\t0\t1\n",
            "1\t0.294118\t0.84\t0.52459\t0\t0\t0.490313\t0.0243382\t0.333333\t1\t0\n",
            "1\t0.352941\t0.625\t0.622951\t0\t0\t0.503726\t0.0183604\t0.55\t1\t0\n",
            "1\t0.117647\t0.525\t0.655738\t0.45\t0.225768\t0.502235\t0.270282\t0.133333\t1\t0\n",
            "1\t0.235294\t0.655\t0.557377\t0.21\t0.196217\t0.493294\t0.0350128\t0.116667\t0\t1\n",
            "1\t0.176471\t0.495\t0.442623\t0.19\t0.101655\t0.38152\t0.0324509\t0.05\t0\t1\n",
            "1\t0.235294\t0.575\t0.590164\t0\t0\t0.4307\t0.127242\t0.416667\t1\t0\n",
            "1\t0.529412\t0.6\t0.590164\t0.22\t0.0661939\t0.309985\t0.279675\t0.45\t0\t1\n",
            "1\t0.411765\t0.53\t0.491803\t0.24\t0\t0.394933\t0.0930828\t0.133333\t1\t0\n",
            "1\t0.117647\t0.54\t0.508197\t0.1\t0.328605\t0.377049\t0.342869\t0.0166667\t0\t1\n",
            "1\t0.176471\t0.565\t0.409836\t0.1\t0.100473\t0.439642\t0.233988\t0.0666667\t0\t1\n",
            "1\t0.117647\t0.87\t0.721311\t0.37\t0.141844\t0.663189\t0.242528\t0.05\t1\t0\n",
            "1\t0.0588235\t0.485\t0.57377\t0.4\t0\t0.567809\t0.059778\t0.15\t0\t1\n",
            "1\t0.235294\t0.66\t0\t0\t0\t0.490313\t0.0956447\t0.0333333\t1\t0\n",
            "1\t0.294118\t0.495\t0.606557\t0.27\t0\t0.432191\t0.0533732\t0.183333\t0\t1\n",
            "1\t0.0588235\t0.73\t0.459016\t0\t0\t0.442623\t0.207515\t0.133333\t0\t1\n",
            "1\t0\t0.73\t0.57377\t0\t0\t0.564829\t0.109308\t0.116667\t1\t0\n",
            "1\t0.117647\t0.525\t0.47541\t0.4\t0.111111\t0.520119\t0.0627669\t0.0666667\t0\t1\n",
            "1\t0.117647\t0.46\t0.622951\t0.2\t0\t0.360656\t0.691716\t0.116667\t0\t1\n",
            "1\t0.470588\t0.62\t0.622951\t0.24\t0.70922\t0.42772\t0.260034\t0.516667\t1\t0\n",
            "1\t0\t0.455\t0.655738\t0\t0\t0.482861\t0.223313\t0.1\t0\t1\n",
            "1\t0\t0.755\t0.737705\t0.46\t0\t0.627422\t0.125107\t0\t1\t0\n",
            "1\t0.411765\t0.935\t0.409836\t0.33\t0.463357\t0.505216\t0.319385\t0.216667\t1\t0\n",
            "1\t0.235294\t0.55\t0.754098\t0\t0\t0.560358\t0.0482494\t0.15\t0\t1\n",
            "1\t0.411765\t0.75\t0.540984\t0.42\t0.404255\t0.517139\t0.273271\t0.35\t0\t1\n",
            "1\t0.352941\t0.645\t0.737705\t0.07\t0.385343\t0.292101\t0.215201\t0.65\t0\t1\n",
            "1\t0.176471\t0.435\t0.491803\t0.18\t0\t0.324888\t0.156277\t0\t0\t1\n",
            "1\t0.0588235\t0.595\t0.442623\t0.13\t0.0591017\t0.33234\t0.0542272\t0.05\t0\t1\n",
            "1\t0.588235\t0.505\t0.704918\t0.37\t0\t0.679583\t0.451751\t0.283333\t1\t0\n",
            "1\t0.352941\t0.62\t0.590164\t0\t0\t0.411326\t0.123826\t0.133333\t1\t0\n",
            "1\t0.529412\t0.61\t0.459016\t0\t0\t0.496274\t0.442357\t0.2\t1\t0\n",
            "1\t0.176471\t0.91\t0.606557\t0\t0\t0.454545\t0.114005\t0.133333\t1\t0\n",
            "1\t0.176471\t0.4\t0\t0\t0\t0\t0.0409906\t0.0166667\t0\t1\n",
            "1\t0.117647\t0.6\t0.622951\t0.37\t0.124113\t0.591654\t0.058497\t0.133333\t0\t1\n",
            "1\t0.176471\t0.625\t0.47541\t0\t0\t0.470939\t0.0311699\t0.05\t0\t1\n",
            "1\t0.352941\t0.52\t0.606557\t0.18\t0.184397\t0.445604\t0.274979\t0.333333\t1\t0\n",
            "1\t0.470588\t0.6\t0\t0\t0\t0.447094\t0.0448335\t0.283333\t1\t0\n",
            "1\t0.117647\t0.355\t0.57377\t0.27\t0\t0.417288\t0.216909\t0.0166667\t0\t1\n",
            "1\t0.0588235\t0.42\t0.52459\t0.23\t0.135934\t0.549925\t0.167805\t0.116667\t0\t1\n",
            "1\t0.705882\t0.7\t0.672131\t0.43\t0.384161\t0.584203\t0.192143\t0.616667\t1\t0\n",
            "1\t0.235294\t0.48\t0.459016\t0.17\t0.0579196\t0.309985\t0.11187\t0.0833333\t0\t1\n",
            "1\t0.176471\t0.495\t0.508197\t0.19\t0.0874704\t0.324888\t0.0858241\t0.0833333\t0\t1\n",
            "1\t0.117647\t0.985\t0.57377\t0.99\t0\t0.517139\t0.212212\t0.683333\t1\t0\n",
            "1\t0.0588235\t0.525\t0.47541\t0\t0\t0.362146\t0.0465414\t0\t0\t1\n",
            "1\t0\t0.615\t0.721311\t0.37\t0\t0.52459\t0.0508113\t0.133333\t0\t1\n",
            "1\t0.117647\t0.595\t0\t0\t0\t0.292101\t0.321947\t0.85\t0\t1\n",
            "1\t0.117647\t0.65\t0.786885\t0\t0\t0.336811\t0.0811272\t0\t0\t1\n",
            "1\t0\t0.525\t0.557377\t0.22\t0\t0.298063\t0.0674637\t0.0166667\t0\t1\n",
            "1\t0.352941\t0.57\t0\t0\t0\t0\t0.0473954\t0.0833333\t0\t1\n",
            "1\t0.352941\t0.46\t0.508197\t0.32\t0.148936\t0.4769\t0.0029889\t0.416667\t0\t1\n",
            "1\t0.0588235\t0.485\t0.557377\t0.21\t0\t0.405365\t0.434244\t0.0166667\t0\t1\n",
            "1\t0.647059\t0.635\t0.868852\t0\t0\t0.581222\t0.0478224\t0.5\t0\t1\n",
            "1\t0.0588235\t0.535\t0.590164\t0.3\t0.0969267\t0.459016\t0.31725\t0.05\t0\t1\n",
            "1\t0.588235\t0.805\t0.557377\t0.23\t0.156028\t0.38003\t0.105892\t0.433333\t1\t0\n",
            "1\t0.529412\t0.82\t0.639344\t0\t0\t0.488823\t0.029889\t0.4\t1\t0\n",
            "1\t0.470588\t0.56\t0.590164\t0\t0\t0.351714\t0.325363\t0.616667\t0\t1\n",
            "1\t0.294118\t0.68\t0.672131\t0\t0\t0\t0.239966\t0.8\t0\t1\n",
            "1\t0.176471\t0.965\t0.57377\t0.31\t0\t0.520119\t0.0695986\t0.0666667\t1\t0\n",
            "1\t0.235294\t0.58\t0.590164\t0.12\t0.102837\t0.329359\t0.164389\t0.266667\t0\t1\n",
            "1\t0.0588235\t0.63\t0.491803\t0\t0\t0.448584\t0.115713\t0.433333\t1\t0\n",
            "1\t0.235294\t0.73\t0.754098\t0\t0\t0.464978\t0.19684\t0.666667\t1\t0\n",
            "1\t0.0588235\t0.555\t0.508197\t0.13\t0.21513\t0.357675\t0.0256191\t0.0333333\t0\t1\n",
            "1\t0.352941\t0.755\t0.508197\t0.31\t0.141844\t0.529061\t0.262169\t0.116667\t0\t1\n",
            "1\t0.176471\t0.37\t0.557377\t0.28\t0.0531915\t0.442623\t0.0918019\t0.0333333\t0\t1\n",
            "1\t0.0588235\t0.505\t0.409836\t0.15\t0.0425532\t0.360656\t0.191289\t0.0833333\t0\t1\n",
            "1\t0.0588235\t0.485\t0.52459\t0.19\t0.0969267\t0.271237\t0.0943638\t0\t0\t1\n",
            "1\t0.352941\t0.615\t0.590164\t0.45\t0.271868\t0.500745\t0.279675\t0.216667\t0\t1\n",
            "1\t0.647059\t0.515\t0.557377\t0.4\t0\t0.688525\t0.0204953\t0.35\t0\t1\n",
            "1\t0.0588235\t0.4\t0.606557\t0.11\t0.070922\t0.447094\t0.191716\t0.0166667\t0\t1\n",
            "1\t0.529412\t0.77\t0.639344\t0.3\t0.118203\t0.460507\t0.0367208\t0.4\t0\t1\n",
            "1\t0.705882\t0.7\t0.696721\t0.33\t0\t0.557377\t0.0708796\t0.333333\t0\t1\n",
            "1\t0\t0.565\t0.622951\t0\t0\t0.496274\t0.0853971\t0.0333333\t1\t0\n",
            "1\t0\t0.475\t0.696721\t0.25\t0.0425532\t0.557377\t0.0721605\t0.05\t1\t0\n",
            "1\t0.294118\t0.55\t0.557377\t0\t0\t0.387481\t0.0913749\t0.15\t0\t1\n",
            "1\t0.529412\t0.78\t0.704918\t0.28\t0.183215\t0.511177\t0.474381\t0.35\t1\t0\n",
            "1\t0.588235\t0.61\t0.639344\t0.31\t0\t0.411326\t0.185312\t0.4\t0\t1\n",
            "1\t0.294118\t0.83\t0.590164\t0.19\t0.206856\t0.384501\t0.217336\t0.5\t1\t0\n",
            "1\t0.235294\t0.73\t0.639344\t0\t0\t0.57377\t0.188728\t0.766667\t1\t0\n",
            "1\t0\t0.595\t0.540984\t0.27\t0\t0.578241\t0.0772844\t0.0166667\t0\t1\n",
            "1\t0\t0.525\t0.52459\t0.41\t0.167849\t0.61848\t0.0405636\t0.0166667\t0\t1\n",
            "1\t0.117647\t0.59\t0.655738\t0\t0\t0.639344\t0.262596\t0\t1\t0\n",
            "1\t0.294118\t0.62\t0.606557\t0\t0\t0.506706\t0.0606319\t0.283333\t1\t0\n",
            "1\t0.176471\t0.815\t0.57377\t0.18\t0.124113\t0.470939\t0.0811272\t0.116667\t1\t0\n",
            "1\t0.352941\t0.95\t0.754098\t0\t0\t0.529061\t0.0853971\t0.75\t1\t0\n",
            "1\t0.176471\t0.495\t0.655738\t0.11\t0.0756501\t0.28763\t0.087959\t0.15\t0\t1\n",
            "1\t0.352941\t0.4\t0.655738\t0.36\t0\t0.593145\t0.0422716\t0.116667\t0\t1\n",
            "1\t0.117647\t0.645\t0.606557\t0.26\t0.242317\t0.494784\t0.219044\t0.0666667\t0\t1\n",
            "1\t0.235294\t0.73\t0.696721\t0.27\t0.118203\t0.4307\t0.0473954\t0.1\t0\t1\n",
            "1\t0.411765\t0.47\t0.52459\t0.25\t0.0933806\t0.496274\t0.28181\t0.333333\t0\t1\n",
            "1\t0.0588235\t0.54\t0.491803\t0.46\t0.210402\t0.529061\t0.143894\t0.05\t0\t1\n",
            "1\t0.235294\t0.585\t0.508197\t0.12\t0\t0.442623\t0.12895\t0.15\t1\t0\n",
            "1\t0.0588235\t0.605\t0.639344\t0.39\t0.0874704\t0.581222\t0.0781383\t0.116667\t0\t1\n",
            "1\t0\t0.9\t0.540984\t0.39\t0\t0.625931\t0.774979\t0.0666667\t1\t0\n",
            "1\t0\t0.905\t0.721311\t0.44\t0.602837\t0.645306\t0.0614859\t0.0833333\t1\t0\n",
            "1\t0.0588235\t0.395\t0.614754\t0.3\t0\t0.4769\t0.135781\t0.0166667\t0\t1\n",
            "1\t0.176471\t0.88\t0.704918\t0.27\t0.184397\t0.496274\t0.459436\t0.516667\t1\t0\n",
            "1\t0.352941\t0.435\t0.655738\t0\t0\t0.345753\t0.00256191\t0.183333\t0\t1\n",
            "1\t0.117647\t0.61\t0.491803\t0.18\t0.125296\t0.444113\t0.272844\t0.0166667\t0\t1\n",
            "1\t0\t0.37\t0.42623\t0.1\t0.0425532\t0.414307\t0.0815542\t0.0166667\t0\t1\n",
            "1\t0.352941\t0\t0.557377\t0.41\t0\t0.581222\t0.277114\t0.333333\t1\t0\n",
            "1\t0.176471\t0.39\t0.409836\t0.32\t0.104019\t0.461997\t0.0725875\t0.0833333\t1\t0\n",
            "1\t0.235294\t0.74\t0.491803\t0.27\t0.375887\t0.460507\t0.030743\t0.133333\t1\t0\n",
            "1\t0.0588235\t0\t0.393443\t0.2\t0\t0.368107\t0.0264731\t0.0166667\t0\t1\n",
            "1\t0.0588235\t0.545\t0.459016\t0.21\t0.159574\t0.375559\t0.322374\t0.0333333\t0\t1\n",
            "1\t0.117647\t0.635\t0.377049\t0.21\t0.395981\t0.512668\t0.0418446\t0.0166667\t0\t1\n",
            "1\t0.470588\t0.6\t0.704918\t0\t0\t0.423249\t0.0772844\t0.0166667\t1\t0\n",
            "1\t0.176471\t0.58\t0.606557\t0.15\t0.124113\t0.391952\t0.0123826\t0.05\t0\t1\n",
            "1\t0.117647\t0.79\t0.737705\t0\t0\t0.470939\t0.310418\t0.75\t1\t0\n",
            "1\t0.411765\t0.71\t0.737705\t0.24\t0.567376\t0.453055\t0.0213493\t0.366667\t1\t0\n",
            "1\t0.235294\t0.865\t0.57377\t0.14\t0.198582\t0.442623\t0.120837\t0.2\t1\t0\n",
            "1\t0.0588235\t0.455\t0.52459\t0.24\t0\t0.435171\t0.0486763\t0\t0\t1\n",
            "1\t0.764706\t0.53\t0.590164\t0.54\t0\t0.545455\t0.0426985\t0.4\t0\t1\n",
            "1\t0\t0.63\t0.704918\t0.27\t0.141844\t0.408346\t0.186593\t0\t0\t1\n",
            "1\t0.0588235\t0.395\t0.491803\t0.42\t0.0567376\t0.648286\t0.256191\t0.0333333\t0\t1\n",
            "1\t0\t0.565\t0.655738\t0.16\t0\t0.461997\t0.33988\t0\t0\t1\n",
            "1\t0.176471\t0.535\t0.508197\t0.13\t0.0567376\t0.341282\t0.256191\t0.0333333\t1\t0\n",
            "1\t0\t0.59\t0.52459\t0.23\t0.105201\t0\t0.705807\t0\t0\t1\n",
            "1\t0.235294\t0.57\t0.52459\t0\t0\t0.4307\t0.0204953\t0.05\t0\t1\n",
            "1\t0.0588235\t0.72\t0.672131\t0.4\t0\t0.615499\t0.225875\t0.116667\t0\t1\n",
            "1\t0.176471\t0.64\t0.639344\t0\t0\t0.314456\t0.0811272\t0.566667\t0\t1\n",
            "1\t0.294118\t0.54\t0.590164\t0.43\t0.0886525\t0.538003\t0.0789923\t0.2\t0\t1\n",
            "1\t0.117647\t0.67\t0.57377\t0\t0\t0.4307\t0.198121\t0.0333333\t1\t0\n",
            "1\t0.470588\t0.715\t0.540984\t0\t0\t0.520119\t0.0217763\t0.333333\t1\t0\n",
            "1\t0.411765\t0.98\t0.737705\t0\t0\t0.593145\t0.159266\t0.333333\t1\t0\n",
            "1\t0.117647\t0.56\t0.614754\t0.32\t0\t0.532042\t0.029889\t0\t0\t1\n",
            "1\t0.117647\t0.56\t0.540984\t0.22\t0\t0.372578\t0.0977797\t0.05\t0\t1\n",
            "1\t0.117647\t0.425\t0.532787\t0\t0\t0.590164\t0.363792\t0.1\t0\t1\n",
            "1\t0.0588235\t0.585\t0.491803\t0.23\t0.125296\t0.503726\t0.16567\t0.1\t0\t1\n",
            "1\t0.0588235\t0.695\t0.377049\t0.19\t0.0981087\t0.42772\t0.245944\t0.0166667\t0\t1\n",
            "1\t0\t0.6\t0.606557\t0.18\t0.0744681\t0.454545\t0.088386\t0.0833333\t0\t1\n",
            "1\t0\t0.805\t0.409836\t0\t0\t0.326379\t0.0751494\t0.733333\t0\t1\n",
            "1\t0.176471\t0.645\t0.754098\t0.49\t0.183215\t0.542474\t0.380017\t0.183333\t1\t0\n",
            "1\t0.0588235\t0.425\t0.540984\t0.29\t0\t0.396423\t0.116567\t0.166667\t0\t1\n",
            "1\t0\t0.655\t0.721311\t0\t0\t0.470939\t0.283945\t0.183333\t1\t0\n",
            "1\t0.0588235\t0.69\t0.672131\t0\t0\t0.597615\t0.0674637\t0.116667\t0\t1\n",
            "1\t0.352941\t0.825\t0.557377\t0.26\t0.198582\t0.500745\t0.236123\t0.466667\t0\t1\n",
            "1\t0.588235\t0.61\t0.557377\t0\t0\t0.464978\t0.0768574\t0.333333\t0\t1\n",
            "1\t0\t0.285\t0.491803\t0\t0\t0.323398\t0.280529\t0.766667\t0\t1\n",
            "1\t0.0588235\t0.56\t0.655738\t0.45\t0.156028\t0.518629\t0.059351\t0.05\t0\t1\n",
            "1\t0.0588235\t0.58\t0.639344\t0.29\t0.212766\t0.538003\t0.17848\t0.0666667\t0\t1\n",
            "1\t0\t0.555\t0.532787\t0\t0\t0.366617\t0.248506\t0.166667\t0\t1\n",
            "1\t0.294118\t0.495\t0.442623\t0.28\t0.0981087\t0.506706\t0.179761\t0.15\t0\t1\n",
            "1\t0\t0.47\t0\t0\t0\t0\t0.0760034\t0.0666667\t0\t1\n",
            "1\t0.294118\t0.58\t0.606557\t0.29\t0\t0.481371\t0.248506\t0.233333\t1\t0\n",
            "1\t0.235294\t0.57\t0.532787\t0\t0\t0.326379\t0.151153\t0.266667\t0\t1\n",
            "1\t0.588235\t0.54\t0.540984\t0\t0\t0.482861\t0.0828352\t0.35\t1\t0\n",
            "1\t0.294118\t0.555\t0.590164\t0.28\t0\t0.356185\t0.140478\t0.1\t0\t1\n",
            "1\t0\t0.59\t0.688525\t0.47\t0.271868\t0.682563\t0.201964\t0.166667\t1\t0\n",
            "1\t0.117647\t0.41\t0.42623\t0.22\t0.135934\t0.424739\t0.692143\t0.0666667\t0\t1\n",
            "1\t0.176471\t0.555\t0.47541\t0.31\t0.0520095\t0.439642\t0.150299\t0.0166667\t0\t1\n",
            "1\t0.294118\t0.66\t0.655738\t0\t0\t0.399404\t0.0461144\t0.8\t0\t1\n",
            "1\t0.529412\t0.67\t0.606557\t0.33\t0.070922\t0.385991\t0.163108\t1\t0\t1\n",
            "1\t0.117647\t0.5\t0.557377\t0.25\t0.0839243\t0.57377\t0.105038\t0.0833333\t0\t1\n",
            "1\t0.764706\t0.52\t0.590164\t0\t0\t0.464978\t0.165243\t0.283333\t1\t0\n",
            "1\t0.117647\t0.5\t0.57377\t0.52\t0.0673759\t0.603577\t0.255764\t0.0666667\t0\t1\n",
            "1\t0\t0.42\t0.672131\t0.31\t0.147754\t0.5693\t0.0661827\t0.0333333\t0\t1\n",
            "1\t0.117647\t0.505\t0.47541\t0.35\t0.106383\t0.324888\t0.0328779\t0.0166667\t0\t1\n",
            "1\t0.294118\t0.735\t0.639344\t0\t0\t0.502235\t0.059778\t0.733333\t0\t1\n",
            "1\t0.529412\t0.855\t0.901639\t0.24\t0.283688\t0.676602\t0.274552\t0.55\t1\t0\n",
            "1\t0.705882\t0.755\t0.57377\t0.4\t0.320331\t0.622951\t0.283518\t0.283333\t1\t0\n",
            "1\t0.294118\t0.81\t0.852459\t0\t0\t0.561848\t0.0311699\t0.516667\t1\t0\n",
            "1\t0.529412\t0.62\t0.57377\t0.33\t0.475177\t0.527571\t0.087105\t0.216667\t0\t1\n",
            "1\t0.0588235\t0.835\t0.606557\t0.17\t0.170213\t0.348733\t0.157558\t0.2\t1\t0\n",
            "1\t0.294118\t0.64\t0.655738\t0\t0\t0.515648\t0.028181\t0.4\t0\t1\n",
            "1\t0.411765\t0.31\t0.639344\t0\t0\t0.485842\t0.133646\t0.333333\t0\t1\n",
            "1\t0.0588235\t0.495\t0.590164\t0.3\t0.0212766\t0.575261\t0.142613\t0\t0\t1\n",
            "1\t0.470588\t0.88\t0.737705\t0.34\t0.35461\t0.502235\t0.166097\t0.616667\t1\t0\n",
            "1\t0\t0.49\t0.672131\t0.15\t0.0992908\t0.375559\t0.0943638\t0.0166667\t0\t1\n",
            "1\t0.0588235\t0.745\t0.557377\t0.29\t0.150118\t0.436662\t0.115713\t0.35\t1\t0\n",
            "1\t0.294118\t0.56\t0.540984\t0\t0\t0.563338\t0.0781383\t0.333333\t1\t0\n",
            "1\t0\t0.9\t0.737705\t0.26\t0.106383\t0.543964\t0.100769\t0.233333\t1\t0\n",
            "1\t0.0588235\t0\t0.557377\t0.35\t0\t0.4769\t0.132792\t0.0166667\t0\t1\n",
            "1\t0.294118\t0.63\t0.639344\t0.27\t0.0260047\t0.441133\t0.154142\t0.316667\t0\t1\n",
            "1\t0.235294\t0.725\t0.672131\t0.18\t0\t0.484352\t0.0670367\t0.816667\t1\t0\n",
            "1\t0.411765\t0.57\t0.540984\t0\t0\t0.488823\t0.0768574\t0.35\t1\t0\n",
            "1\t0.0588235\t0.43\t0.540984\t0.52\t0.0768322\t0.615499\t0.358241\t0.133333\t0\t1\n",
            "1\t0.0588235\t0.865\t0.606557\t0\t0\t0.548435\t0.00426985\t0.283333\t1\t0\n",
            "1\t0.352941\t0.54\t0.360656\t0.2\t0.153664\t0.357675\t0.313834\t0.233333\t0\t1\n",
            "1\t0.294118\t0.79\t0.688525\t0.41\t0.248227\t0.587183\t0.135354\t0.133333\t1\t0\n",
            "1\t0.117647\t0.61\t0.622951\t0.27\t0.236407\t0.535022\t0.172929\t0.0833333\t0\t1\n",
            "1\t0\t0.685\t0.327869\t0.35\t0.198582\t0.642325\t0.943638\t0.2\t1\t0\n",
            "1\t0.411765\t0.645\t0.557377\t0.49\t0.147754\t0.57377\t0.154142\t0.366667\t1\t0\n",
            "1\t0.352941\t0.67\t0.57377\t0.23\t0.153664\t0.527571\t0.198121\t0.133333\t1\t0\n",
            "1\t0.117647\t0.57\t0.557377\t0.22\t0\t0.42772\t0.0059778\t0.0666667\t0\t1\n",
            "1\t0\t0.52\t0.52459\t0.37\t0.0756501\t0.500745\t0.184458\t0.0166667\t1\t0\n",
            "1\t0.176471\t0.51\t0.606557\t0\t0\t0.439642\t0.0183604\t0.183333\t0\t1\n",
            "1\t0.294118\t0.385\t0.672131\t0.41\t0.0496454\t0.533532\t0.0333049\t0.233333\t0\t1\n",
            "1\t0\t0.455\t0.557377\t0.32\t0.248227\t0.594635\t0.129377\t0.0666667\t0\t1\n",
            "1\t0.235294\t0.545\t0.52459\t0.44\t0.117021\t0.518629\t0.353117\t0.0833333\t1\t0\n",
            "1\t0.117647\t0.37\t0\t0\t0\t0\t0.0102477\t0.0166667\t0\t1\n",
            "1\t0.117647\t0.56\t0.639344\t0.5\t0.165485\t0.587183\t0.0414176\t0.05\t0\t1\n",
            "1\t0.117647\t0.54\t0.655738\t0\t0\t0.402385\t0.0772844\t0.516667\t1\t0\n",
            "1\t0.411765\t0.975\t0.57377\t0.33\t0.171395\t0.374069\t0.0362938\t0.566667\t1\t0\n",
            "1\t0.176471\t0.58\t0\t0\t0\t0.350224\t0.0465414\t0.0333333\t0\t1\n",
            "1\t0\t0.42\t0.52459\t0.22\t0.0780142\t0.533532\t0.199402\t0\t0\t1\n",
            "1\t0\t0.685\t0.57377\t0.38\t0\t0.494784\t0.0392827\t0.0166667\t0\t1\n",
            "1\t0\t0.465\t0.819672\t0.39\t0.0851064\t0.646796\t0.402647\t0.233333\t0\t1\n",
            "1\t0.294118\t0.685\t0.885246\t0\t0\t0.727273\t0.0636208\t0.266667\t1\t0\n",
            "1\t0.529412\t0.595\t0.655738\t0.35\t0\t0.432191\t0.0789923\t0.133333\t1\t0\n",
            "1\t0.176471\t0.48\t0.639344\t0.39\t0\t0.555887\t0.0683177\t0.316667\t0\t1\n",
            "1\t0.117647\t0.28\t0.459016\t0.28\t0.0531915\t0.360656\t0.108454\t0.0166667\t0\t1\n",
            "1\t0.117647\t0.545\t0.754098\t0\t0\t0.636364\t0.327498\t0.55\t0\t1\n",
            "1\t0.176471\t0.45\t0.639344\t0\t0\t0.636364\t0.20538\t0\t0\t1\n",
            "1\t0.0588235\t0.51\t0.606557\t0\t0\t0.588674\t0.0918019\t0.35\t1\t0\n",
            "1\t0.352941\t0.515\t0.590164\t0.32\t0.224586\t0.561848\t0.105038\t0.566667\t0\t1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gathering Training Data**"
      ],
      "metadata": {
        "id": "n7oM5nONvT30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the diabetes dataset (diabetes.txt), I divided all 768 tuples into two parts: let the first 75% tuples be the training data and the rest 25% of the tuples be the final testing data."
      ],
      "metadata": {
        "id": "BzcRNj0-vcZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ANN learning/ training method**"
      ],
      "metadata": {
        "id": "QrdwLhiOwK69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I built an artificial neural network using a genetic algorithm to optimize the weights in each layer of the training model to predict the unseen diabetes dataset.\n",
        "\n",
        "Initially, for a constant number of populations (e.g., 20) is set, the sequential model will generate the weights of all layers for each population randomly. Then the training data will be fed into the training model, and the predicting process begins. After the fitness calculation, which compares the true output and the predicted output, the program will update the maximum fitness value for the final training since its weights are the optimal ones and could likely yield higher accuracy in the final training stage. This process will continue going on until the maximum generation is met.\n",
        "\n",
        "After optimizing the weight matrix, the optimal matrix will be set to the ANN model and be ready to generalize the testing data. In one particular example, I set the model with one input layer, three hidden layers with eight, six, and six neurons, respectively, and finally, one neuron for the output layer."
      ],
      "metadata": {
        "id": "EYD0GfIdwSxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensure the ANN can generalize well."
      ],
      "metadata": {
        "id": "EOQ2Q-FJwaSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although it is tough to promise that the ANN can always generalize the testing data well, applying a genetic algorithm could help raise the accuracy rate by preventing the model from being trapped in a local minimum scenario.\n",
        "\n",
        "The three key parts of the genetic algorithm (GA) is selection, crossover, and mutation. First, the mechanism selects the elite parents to the gene pool (an array that keeps track of the best matrix of weights) to realize the elitism. Second, the crossover is implemented. Among the best genes (weighted matrix), the mechanism selects two genes randomly and recombines them in a certain approach defined in the provided python code. For instance, in this case, I randomly choose a split point for the elite gene 1 and gene 2. Then I concatenate the second part of gene 2 to the first part of gene 1 and perform the reverse method for the remaining parts of the two genes. As a result, I have two recombined possibly elite genes. Third, a mutation might occur since it happens on a random basis. Each generation, after performing the crossover, the mechanism will generate a random number between 0 to 1. If the randomly generated number is less than or equal to 0.05, a particular part of the weighted matrix, which is also defined randomly, will be multiplied by another random number between 2 and 5. By slightly scaling some values in the weighted matrix can help achieve the mutation process to prevent the ANN model from being trained in the wrong direction (yielding a lower training accuracy rate).\n",
        "\n",
        "There are so many factors that could affect the performance of an ANN. Including the number of layers and number of neurons in each layer, learning rate, optimization function, loss function, and others. In a genetic algorithm, population size, number of generations, crossover rate, mutation rate, and probability also need to be considered when building the ANN. For instance, in theory, if the mutation rate and its mutation probability are high, the model should come up with a weighted matrix with a higher variance of values than those with a lower mutation rate and lower mutation probability. Its main goal is to try to solve the drawback of the traditional gradient descent learning algorithm by ensuring having various possible weighted matrices instead of possibly training a model in a wrong approach without getting an optimal solution for any problem."
      ],
      "metadata": {
        "id": "BeSWM9iOwmJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Performance comparison of different training methods**"
      ],
      "metadata": {
        "id": "lAxka33pws-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare different training algorithms or methods, I tested ANN models with SGD (Stochastic Gradient Descent), RMSprop (Root Mean Square Propagation), and Adam (Adaptive moment estimation) training methods using the Keras model in python. Below are the results of the different algorithms."
      ],
      "metadata": {
        "id": "f8Lr7d5Xw6jr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKFDDUjVhdlt"
      },
      "source": [
        "### **ADAM (Adaptive moment estimation) Training** **Method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BGvLdOLahbkF",
        "outputId": "116b63ae-e0af-4dea-a932-2d5cf19c14d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "18/18 [==============================] - 1s 3ms/step - loss: 0.5305 - accuracy: 0.3438\n",
            "Epoch 2/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.3042 - accuracy: 0.3316\n",
            "Epoch 3/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2401 - accuracy: 0.6267\n",
            "Epoch 4/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2352 - accuracy: 0.6562\n",
            "Epoch 5/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2310 - accuracy: 0.6562\n",
            "Epoch 6/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2272 - accuracy: 0.6562\n",
            "Epoch 7/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.6562\n",
            "Epoch 8/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2205 - accuracy: 0.6562\n",
            "Epoch 9/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2175 - accuracy: 0.6562\n",
            "Epoch 10/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2142 - accuracy: 0.6562\n",
            "Epoch 11/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.6562\n",
            "Epoch 12/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2081 - accuracy: 0.6562\n",
            "Epoch 13/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2053 - accuracy: 0.6562\n",
            "Epoch 14/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2027 - accuracy: 0.6597\n",
            "Epoch 15/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2001 - accuracy: 0.6649\n",
            "Epoch 16/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1963 - accuracy: 0.6910\n",
            "Epoch 17/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1945 - accuracy: 0.6997\n",
            "Epoch 18/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1913 - accuracy: 0.7066\n",
            "Epoch 19/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1896 - accuracy: 0.7170\n",
            "Epoch 20/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1876 - accuracy: 0.7049\n",
            "Epoch 21/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.7188\n",
            "Epoch 22/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1826 - accuracy: 0.7240\n",
            "Epoch 23/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1806 - accuracy: 0.7240\n",
            "Epoch 24/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1802 - accuracy: 0.7257\n",
            "Epoch 25/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1785 - accuracy: 0.7205\n",
            "Epoch 26/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1753 - accuracy: 0.7361\n",
            "Epoch 27/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1732 - accuracy: 0.7378\n",
            "Epoch 28/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.7361\n",
            "Epoch 29/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1706 - accuracy: 0.7431\n",
            "Epoch 30/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1686 - accuracy: 0.7413\n",
            "Epoch 31/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1669 - accuracy: 0.7517\n",
            "Epoch 32/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1664 - accuracy: 0.7569\n",
            "Epoch 33/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1639 - accuracy: 0.7500\n",
            "Epoch 34/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1634 - accuracy: 0.7500\n",
            "Epoch 35/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1621 - accuracy: 0.7517\n",
            "Epoch 36/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1610 - accuracy: 0.7587\n",
            "Epoch 37/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1595 - accuracy: 0.7517\n",
            "Epoch 38/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1613 - accuracy: 0.7552\n",
            "Epoch 39/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1592 - accuracy: 0.7587\n",
            "Epoch 40/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1567 - accuracy: 0.7656\n",
            "Epoch 41/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1574 - accuracy: 0.7743\n",
            "Epoch 42/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1559 - accuracy: 0.7639\n",
            "Epoch 43/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1558 - accuracy: 0.7743\n",
            "Epoch 44/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1543 - accuracy: 0.7691\n",
            "Epoch 45/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1544 - accuracy: 0.7812\n",
            "Epoch 46/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1539 - accuracy: 0.7743\n",
            "Epoch 47/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1537 - accuracy: 0.7795\n",
            "Epoch 48/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1549 - accuracy: 0.7743\n",
            "Epoch 49/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1560 - accuracy: 0.7656\n",
            "Epoch 50/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1541 - accuracy: 0.7708\n",
            "Epoch 51/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1546 - accuracy: 0.7674\n",
            "Epoch 52/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1554 - accuracy: 0.7726\n",
            "Epoch 53/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1551 - accuracy: 0.7760\n",
            "Epoch 54/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1521 - accuracy: 0.7743\n",
            "Epoch 55/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1531 - accuracy: 0.7760\n",
            "Epoch 56/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1551 - accuracy: 0.7674\n",
            "Epoch 57/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1544 - accuracy: 0.7691\n",
            "Epoch 58/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1565 - accuracy: 0.7656\n",
            "Epoch 59/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1558 - accuracy: 0.7847\n",
            "Epoch 60/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1505 - accuracy: 0.7795\n",
            "Epoch 61/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1532 - accuracy: 0.7830\n",
            "Epoch 62/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1528 - accuracy: 0.7708\n",
            "Epoch 63/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1546 - accuracy: 0.7760\n",
            "Epoch 64/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1519 - accuracy: 0.7778\n",
            "Epoch 65/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1532 - accuracy: 0.7760\n",
            "Epoch 66/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1530 - accuracy: 0.7639\n",
            "Epoch 67/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1533 - accuracy: 0.7674\n",
            "Epoch 68/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1513 - accuracy: 0.7743\n",
            "Epoch 69/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1510 - accuracy: 0.7760\n",
            "Epoch 70/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1511 - accuracy: 0.7778\n",
            "Epoch 71/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1510 - accuracy: 0.7795\n",
            "Epoch 72/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1520 - accuracy: 0.7795\n",
            "Epoch 73/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1534 - accuracy: 0.7882\n",
            "Epoch 74/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1514 - accuracy: 0.7847\n",
            "Epoch 75/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1516 - accuracy: 0.7760\n",
            "Epoch 76/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1532 - accuracy: 0.7743\n",
            "Epoch 77/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1524 - accuracy: 0.7674\n",
            "Epoch 78/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1512 - accuracy: 0.7778\n",
            "Epoch 79/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1510 - accuracy: 0.7830\n",
            "Epoch 80/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1514 - accuracy: 0.7743\n",
            "Epoch 81/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1507 - accuracy: 0.7778\n",
            "Epoch 82/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1521 - accuracy: 0.7708\n",
            "Epoch 83/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1540 - accuracy: 0.7795\n",
            "Epoch 84/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1536 - accuracy: 0.7847\n",
            "Epoch 85/150\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1511 - accuracy: 0.7847\n",
            "Epoch 86/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1515 - accuracy: 0.7795\n",
            "Epoch 87/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1507 - accuracy: 0.7743\n",
            "Epoch 88/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1524 - accuracy: 0.7795\n",
            "Epoch 89/150\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1525 - accuracy: 0.7708\n",
            "Epoch 90/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1517 - accuracy: 0.7778\n",
            "Epoch 91/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1501 - accuracy: 0.7795\n",
            "Epoch 92/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1507 - accuracy: 0.7812\n",
            "Epoch 93/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1509 - accuracy: 0.7830\n",
            "Epoch 94/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1507 - accuracy: 0.7795\n",
            "Epoch 95/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1515 - accuracy: 0.7743\n",
            "Epoch 96/150\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1516 - accuracy: 0.7760\n",
            "Epoch 97/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1510 - accuracy: 0.7778\n",
            "Epoch 98/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.7726\n",
            "Epoch 99/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1506 - accuracy: 0.7708\n",
            "Epoch 100/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1504 - accuracy: 0.7778\n",
            "Epoch 101/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1504 - accuracy: 0.7865\n",
            "Epoch 102/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1508 - accuracy: 0.7778\n",
            "Epoch 103/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1522 - accuracy: 0.7778\n",
            "Epoch 104/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1514 - accuracy: 0.7812\n",
            "Epoch 105/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1509 - accuracy: 0.7743\n",
            "Epoch 106/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1523 - accuracy: 0.7795\n",
            "Epoch 107/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1519 - accuracy: 0.7726\n",
            "Epoch 108/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1504 - accuracy: 0.7795\n",
            "Epoch 109/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1503 - accuracy: 0.7726\n",
            "Epoch 110/150\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1501 - accuracy: 0.7760\n",
            "Epoch 111/150\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1521 - accuracy: 0.7691\n",
            "Epoch 112/150\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1537 - accuracy: 0.7743\n",
            "Epoch 113/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1516 - accuracy: 0.7726\n",
            "Epoch 114/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1507 - accuracy: 0.7708\n",
            "Epoch 115/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1509 - accuracy: 0.7812\n",
            "Epoch 116/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1522 - accuracy: 0.7847\n",
            "Epoch 117/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1524 - accuracy: 0.7656\n",
            "Epoch 118/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1497 - accuracy: 0.7760\n",
            "Epoch 119/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1501 - accuracy: 0.7760\n",
            "Epoch 120/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1509 - accuracy: 0.7760\n",
            "Epoch 121/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1508 - accuracy: 0.7795\n",
            "Epoch 122/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1516 - accuracy: 0.7830\n",
            "Epoch 123/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1525 - accuracy: 0.7743\n",
            "Epoch 124/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1498 - accuracy: 0.7778\n",
            "Epoch 125/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1496 - accuracy: 0.7760\n",
            "Epoch 126/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1497 - accuracy: 0.7743\n",
            "Epoch 127/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1509 - accuracy: 0.7830\n",
            "Epoch 128/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1522 - accuracy: 0.7830\n",
            "Epoch 129/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1530 - accuracy: 0.7726\n",
            "Epoch 130/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1508 - accuracy: 0.7812\n",
            "Epoch 131/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1496 - accuracy: 0.7830\n",
            "Epoch 132/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1496 - accuracy: 0.7795\n",
            "Epoch 133/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1496 - accuracy: 0.7743\n",
            "Epoch 134/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1522 - accuracy: 0.7674\n",
            "Epoch 135/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1506 - accuracy: 0.7830\n",
            "Epoch 136/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1514 - accuracy: 0.7760\n",
            "Epoch 137/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1529 - accuracy: 0.7674\n",
            "Epoch 138/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1512 - accuracy: 0.7830\n",
            "Epoch 139/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1532 - accuracy: 0.7795\n",
            "Epoch 140/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1509 - accuracy: 0.7778\n",
            "Epoch 141/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1495 - accuracy: 0.7795\n",
            "Epoch 142/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1497 - accuracy: 0.7778\n",
            "Epoch 143/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1513 - accuracy: 0.7847\n",
            "Epoch 144/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1497 - accuracy: 0.7760\n",
            "Epoch 145/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1493 - accuracy: 0.7830\n",
            "Epoch 146/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1511 - accuracy: 0.7760\n",
            "Epoch 147/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1502 - accuracy: 0.7795\n",
            "Epoch 148/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1506 - accuracy: 0.7812\n",
            "Epoch 149/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1504 - accuracy: 0.7778\n",
            "Epoch 150/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1495 - accuracy: 0.7795\n",
            "6/6 - 0s - loss: 0.1645 - accuracy: 0.7708 - 107ms/epoch - 18ms/step\n",
            "\n",
            "Test accuracy: 0.7708333134651184\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG,\n",
        "                    format='%(asctime)s %(message)s',\n",
        "                    handlers=[logging.FileHandler(\"gnn.log\"),\n",
        "                              logging.StreamHandler()])\n",
        "\n",
        "# Read Data\n",
        "df = pd.read_table('./diabetes.txt',header=None,encoding='gb2312',sep='\\t')\n",
        "df.astype(float)\n",
        "# remove redundant col which is the opposite value of the 10th col\n",
        "df.pop(10)\n",
        "# remove first col of bias = 1\n",
        "df.pop(0)\n",
        "# the label column\n",
        "label = df.pop(9)\n",
        "\n",
        "# train feature\n",
        "train_feature = df[:576]\n",
        "# train label\n",
        "train_label = label[:576]\n",
        "# test feature\n",
        "test_feature = df[576:]\n",
        "# test label\n",
        "test_label = label[576:]\n",
        "\n",
        "model1 = Sequential([\n",
        "    Dense(6, input_shape=(8,), activation='relu', bias_initializer='ones', kernel_initializer='random_uniform'),\n",
        "    Dense(6, activation='relu'),\n",
        "    Dense(4, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "\n",
        "model1.compile(optimizer=optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epochs = 150\n",
        "history1 = model1.fit(train_feature.values, train_label.values, epochs=epochs)\n",
        "\n",
        "plt.plot(history1.history['accuracy'])\n",
        "plt.plot(history1.history['loss'])\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "test_loss, test_acc = model1.evaluate(test_feature,  test_label, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rGhTNCKnJ1n"
      },
      "source": [
        "### **SGD (Stochastic Gradient Descent) Training** **Method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eNyUE3HXnnsd",
        "outputId": "a6024dd6-060a-4f7c-f8c0-86d8f50edeaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "18/18 [==============================] - 1s 3ms/step - loss: 0.4321 - accuracy: 0.6562\n",
            "Epoch 2/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2577 - accuracy: 0.6562\n",
            "Epoch 3/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2409 - accuracy: 0.6562\n",
            "Epoch 4/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2331 - accuracy: 0.6562\n",
            "Epoch 5/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2291 - accuracy: 0.6562\n",
            "Epoch 6/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2273 - accuracy: 0.6562\n",
            "Epoch 7/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2264 - accuracy: 0.6562\n",
            "Epoch 8/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2260 - accuracy: 0.6562\n",
            "Epoch 9/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.2257 - accuracy: 0.6562\n",
            "Epoch 10/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.6562\n",
            "Epoch 11/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.6562\n",
            "Epoch 12/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.6562\n",
            "Epoch 13/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.6562\n",
            "Epoch 14/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.6562\n",
            "Epoch 15/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.6562\n",
            "Epoch 16/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.6562\n",
            "Epoch 17/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2257 - accuracy: 0.6562\n",
            "Epoch 18/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.6562\n",
            "Epoch 19/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.6562\n",
            "Epoch 20/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.6562\n",
            "Epoch 21/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.6562\n",
            "Epoch 22/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2256 - accuracy: 0.6562\n",
            "Epoch 23/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.6562\n",
            "Epoch 24/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.6562\n",
            "Epoch 25/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.6562\n",
            "Epoch 26/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.6562\n",
            "Epoch 27/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.6562\n",
            "Epoch 28/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.6562\n",
            "Epoch 29/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.6562\n",
            "Epoch 30/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.6562\n",
            "Epoch 31/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.6562\n",
            "Epoch 32/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.6562\n",
            "Epoch 33/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.6562\n",
            "Epoch 34/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.6562\n",
            "Epoch 35/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.6562\n",
            "Epoch 36/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2255 - accuracy: 0.6562\n",
            "Epoch 37/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.6562\n",
            "Epoch 38/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.6562\n",
            "Epoch 39/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2253 - accuracy: 0.6562\n",
            "Epoch 40/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.6562\n",
            "Epoch 41/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.6562\n",
            "Epoch 42/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.6562\n",
            "Epoch 43/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2253 - accuracy: 0.6562\n",
            "Epoch 44/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.6562\n",
            "Epoch 45/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.6562\n",
            "Epoch 46/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.6562\n",
            "Epoch 47/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2253 - accuracy: 0.6562\n",
            "Epoch 48/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2254 - accuracy: 0.6562\n",
            "Epoch 49/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2253 - accuracy: 0.6562\n",
            "Epoch 50/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.6562\n",
            "Epoch 51/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.6562\n",
            "Epoch 52/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.6562\n",
            "Epoch 53/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2253 - accuracy: 0.6562\n",
            "Epoch 54/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2251 - accuracy: 0.6562\n",
            "Epoch 55/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.6562\n",
            "Epoch 56/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.6562\n",
            "Epoch 57/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.6562\n",
            "Epoch 58/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2251 - accuracy: 0.6562\n",
            "Epoch 59/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2251 - accuracy: 0.6562\n",
            "Epoch 60/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2251 - accuracy: 0.6562\n",
            "Epoch 61/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.6562\n",
            "Epoch 62/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2250 - accuracy: 0.6562\n",
            "Epoch 63/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.6562\n",
            "Epoch 64/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2250 - accuracy: 0.6562\n",
            "Epoch 65/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2251 - accuracy: 0.6562\n",
            "Epoch 66/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2251 - accuracy: 0.6562\n",
            "Epoch 67/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2250 - accuracy: 0.6562\n",
            "Epoch 68/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2250 - accuracy: 0.6562\n",
            "Epoch 69/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2250 - accuracy: 0.6562\n",
            "Epoch 70/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2250 - accuracy: 0.6562\n",
            "Epoch 71/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2249 - accuracy: 0.6562\n",
            "Epoch 72/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2249 - accuracy: 0.6562\n",
            "Epoch 73/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2248 - accuracy: 0.6562\n",
            "Epoch 74/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2249 - accuracy: 0.6562\n",
            "Epoch 75/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2249 - accuracy: 0.6562\n",
            "Epoch 76/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2249 - accuracy: 0.6562\n",
            "Epoch 77/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2248 - accuracy: 0.6562\n",
            "Epoch 78/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2248 - accuracy: 0.6562\n",
            "Epoch 79/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2248 - accuracy: 0.6562\n",
            "Epoch 80/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.6562\n",
            "Epoch 81/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.6562\n",
            "Epoch 82/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.6562\n",
            "Epoch 83/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.6562\n",
            "Epoch 84/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2247 - accuracy: 0.6562\n",
            "Epoch 85/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2245 - accuracy: 0.6562\n",
            "Epoch 86/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2246 - accuracy: 0.6562\n",
            "Epoch 87/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2245 - accuracy: 0.6562\n",
            "Epoch 88/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2246 - accuracy: 0.6562\n",
            "Epoch 89/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2245 - accuracy: 0.6562\n",
            "Epoch 90/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2245 - accuracy: 0.6562\n",
            "Epoch 91/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2245 - accuracy: 0.6562\n",
            "Epoch 92/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2243 - accuracy: 0.6562\n",
            "Epoch 93/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2243 - accuracy: 0.6562\n",
            "Epoch 94/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2243 - accuracy: 0.6562\n",
            "Epoch 95/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2243 - accuracy: 0.6562\n",
            "Epoch 96/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.6562\n",
            "Epoch 97/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.6562\n",
            "Epoch 98/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2241 - accuracy: 0.6562\n",
            "Epoch 99/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.6562\n",
            "Epoch 100/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2241 - accuracy: 0.6562\n",
            "Epoch 101/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.6562\n",
            "Epoch 102/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 0.6562\n",
            "Epoch 103/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.6562\n",
            "Epoch 104/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2239 - accuracy: 0.6562\n",
            "Epoch 105/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2236 - accuracy: 0.6562\n",
            "Epoch 106/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2189 - accuracy: 0.6562\n",
            "Epoch 107/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2162 - accuracy: 0.6562\n",
            "Epoch 108/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2153 - accuracy: 0.6562\n",
            "Epoch 109/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2147 - accuracy: 0.6562\n",
            "Epoch 110/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2128 - accuracy: 0.6562\n",
            "Epoch 111/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2127 - accuracy: 0.6562\n",
            "Epoch 112/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2117 - accuracy: 0.6562\n",
            "Epoch 113/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2116 - accuracy: 0.6562\n",
            "Epoch 114/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2103 - accuracy: 0.6562\n",
            "Epoch 115/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2097 - accuracy: 0.6562\n",
            "Epoch 116/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2089 - accuracy: 0.6562\n",
            "Epoch 117/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2079 - accuracy: 0.6562\n",
            "Epoch 118/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2069 - accuracy: 0.6562\n",
            "Epoch 119/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2066 - accuracy: 0.6562\n",
            "Epoch 120/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2050 - accuracy: 0.6562\n",
            "Epoch 121/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2039 - accuracy: 0.6562\n",
            "Epoch 122/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2039 - accuracy: 0.6562\n",
            "Epoch 123/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2024 - accuracy: 0.6562\n",
            "Epoch 124/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2014 - accuracy: 0.6562\n",
            "Epoch 125/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2011 - accuracy: 0.6562\n",
            "Epoch 126/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1994 - accuracy: 0.6562\n",
            "Epoch 127/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2000 - accuracy: 0.6562\n",
            "Epoch 128/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1982 - accuracy: 0.6562\n",
            "Epoch 129/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1961 - accuracy: 0.6562\n",
            "Epoch 130/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1952 - accuracy: 0.6562\n",
            "Epoch 131/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1929 - accuracy: 0.6562\n",
            "Epoch 132/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1889 - accuracy: 0.6580\n",
            "Epoch 133/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1852 - accuracy: 0.6806\n",
            "Epoch 134/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1857 - accuracy: 0.7083\n",
            "Epoch 135/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1838 - accuracy: 0.7083\n",
            "Epoch 136/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1819 - accuracy: 0.7118\n",
            "Epoch 137/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1807 - accuracy: 0.7205\n",
            "Epoch 138/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1814 - accuracy: 0.7292\n",
            "Epoch 139/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1793 - accuracy: 0.7361\n",
            "Epoch 140/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1789 - accuracy: 0.7326\n",
            "Epoch 141/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1783 - accuracy: 0.7240\n",
            "Epoch 142/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1759 - accuracy: 0.7326\n",
            "Epoch 143/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1774 - accuracy: 0.7378\n",
            "Epoch 144/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1761 - accuracy: 0.7326\n",
            "Epoch 145/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1738 - accuracy: 0.7361\n",
            "Epoch 146/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1723 - accuracy: 0.7413\n",
            "Epoch 147/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1721 - accuracy: 0.7500\n",
            "Epoch 148/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1708 - accuracy: 0.7552\n",
            "Epoch 149/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1707 - accuracy: 0.7639\n",
            "Epoch 150/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1701 - accuracy: 0.7465\n",
            "6/6 - 0s - loss: 0.1807 - accuracy: 0.7292 - 133ms/epoch - 22ms/step\n",
            "\n",
            "Test accuracy: 0.7291666865348816\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG,\n",
        "                    format='%(asctime)s %(message)s',\n",
        "                    handlers=[logging.FileHandler(\"gnn.log\"),\n",
        "                              logging.StreamHandler()])\n",
        "\n",
        "# Read Data\n",
        "df = pd.read_table('./diabetes.txt',header=None,encoding='gb2312',sep='\\t')\n",
        "df.astype(float)\n",
        "# remove redundant col which is the opposite value of the 10th col\n",
        "df.pop(10)\n",
        "# remove first col of bias = 1\n",
        "df.pop(0)\n",
        "# the label column\n",
        "label = df.pop(9)\n",
        "\n",
        "# train feature\n",
        "train_feature = df[:576]\n",
        "# train label\n",
        "train_label = label[:576]\n",
        "# test feature\n",
        "test_feature = df[576:]\n",
        "# test label\n",
        "test_label = label[576:]\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(6, input_shape=(8,), activation='relu', bias_initializer='ones', kernel_initializer='random_uniform'),\n",
        "    Dense(6, activation='relu'),\n",
        "    Dense(4, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epochs = 150\n",
        "history = model.fit(train_feature.values, train_label.values, epochs=epochs)\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_feature,  test_label, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c65VEa4JoPlI"
      },
      "source": [
        "### **RMSprop (Root Mean Square Propagation)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iDwKpSb3oTbI",
        "outputId": "1006cd69-fa2d-4c41-97b6-8c013432c859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "18/18 [==============================] - 1s 4ms/step - loss: 0.2812 - accuracy: 0.3490\n",
            "Epoch 2/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2418 - accuracy: 0.6493\n",
            "Epoch 3/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2328 - accuracy: 0.6562\n",
            "Epoch 4/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2296 - accuracy: 0.6562\n",
            "Epoch 5/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2279 - accuracy: 0.6562\n",
            "Epoch 6/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2263 - accuracy: 0.6562\n",
            "Epoch 7/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2246 - accuracy: 0.6562\n",
            "Epoch 8/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2236 - accuracy: 0.6562\n",
            "Epoch 9/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2221 - accuracy: 0.6562\n",
            "Epoch 10/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2210 - accuracy: 0.6562\n",
            "Epoch 11/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2195 - accuracy: 0.6562\n",
            "Epoch 12/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2182 - accuracy: 0.6562\n",
            "Epoch 13/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2169 - accuracy: 0.6562\n",
            "Epoch 14/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2155 - accuracy: 0.6562\n",
            "Epoch 15/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2141 - accuracy: 0.6562\n",
            "Epoch 16/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2134 - accuracy: 0.6562\n",
            "Epoch 17/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2120 - accuracy: 0.6562\n",
            "Epoch 18/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2107 - accuracy: 0.6562\n",
            "Epoch 19/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2099 - accuracy: 0.6562\n",
            "Epoch 20/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2086 - accuracy: 0.6580\n",
            "Epoch 21/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2077 - accuracy: 0.6597\n",
            "Epoch 22/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2065 - accuracy: 0.6615\n",
            "Epoch 23/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2057 - accuracy: 0.6667\n",
            "Epoch 24/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2041 - accuracy: 0.6701\n",
            "Epoch 25/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 0.6632\n",
            "Epoch 26/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2022 - accuracy: 0.6701\n",
            "Epoch 27/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2014 - accuracy: 0.6701\n",
            "Epoch 28/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.2001 - accuracy: 0.6771\n",
            "Epoch 29/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1990 - accuracy: 0.6736\n",
            "Epoch 30/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1979 - accuracy: 0.6788\n",
            "Epoch 31/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1967 - accuracy: 0.6910\n",
            "Epoch 32/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1958 - accuracy: 0.6927\n",
            "Epoch 33/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1949 - accuracy: 0.6806\n",
            "Epoch 34/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1933 - accuracy: 0.6944\n",
            "Epoch 35/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1929 - accuracy: 0.6979\n",
            "Epoch 36/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1920 - accuracy: 0.6997\n",
            "Epoch 37/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1912 - accuracy: 0.6997\n",
            "Epoch 38/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1901 - accuracy: 0.7083\n",
            "Epoch 39/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1896 - accuracy: 0.7066\n",
            "Epoch 40/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1885 - accuracy: 0.7066\n",
            "Epoch 41/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1881 - accuracy: 0.7049\n",
            "Epoch 42/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1872 - accuracy: 0.7031\n",
            "Epoch 43/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.7049\n",
            "Epoch 44/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 0.7014\n",
            "Epoch 45/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.7222\n",
            "Epoch 46/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1841 - accuracy: 0.7274\n",
            "Epoch 47/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1836 - accuracy: 0.7170\n",
            "Epoch 48/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1826 - accuracy: 0.7031\n",
            "Epoch 49/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1820 - accuracy: 0.7188\n",
            "Epoch 50/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1810 - accuracy: 0.7205\n",
            "Epoch 51/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1813 - accuracy: 0.7240\n",
            "Epoch 52/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1798 - accuracy: 0.7292\n",
            "Epoch 53/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1797 - accuracy: 0.7153\n",
            "Epoch 54/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1789 - accuracy: 0.7292\n",
            "Epoch 55/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1785 - accuracy: 0.7205\n",
            "Epoch 56/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1776 - accuracy: 0.7257\n",
            "Epoch 57/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1772 - accuracy: 0.7205\n",
            "Epoch 58/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1768 - accuracy: 0.7274\n",
            "Epoch 59/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1755 - accuracy: 0.7396\n",
            "Epoch 60/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1755 - accuracy: 0.7292\n",
            "Epoch 61/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1752 - accuracy: 0.7431\n",
            "Epoch 62/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1750 - accuracy: 0.7309\n",
            "Epoch 63/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1747 - accuracy: 0.7292\n",
            "Epoch 64/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1743 - accuracy: 0.7292\n",
            "Epoch 65/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1737 - accuracy: 0.7413\n",
            "Epoch 66/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1737 - accuracy: 0.7292\n",
            "Epoch 67/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1728 - accuracy: 0.7361\n",
            "Epoch 68/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1725 - accuracy: 0.7344\n",
            "Epoch 69/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1723 - accuracy: 0.7344\n",
            "Epoch 70/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1714 - accuracy: 0.7378\n",
            "Epoch 71/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1714 - accuracy: 0.7431\n",
            "Epoch 72/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1712 - accuracy: 0.7274\n",
            "Epoch 73/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1703 - accuracy: 0.7378\n",
            "Epoch 74/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1701 - accuracy: 0.7396\n",
            "Epoch 75/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1700 - accuracy: 0.7448\n",
            "Epoch 76/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1697 - accuracy: 0.7396\n",
            "Epoch 77/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1690 - accuracy: 0.7448\n",
            "Epoch 78/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1691 - accuracy: 0.7431\n",
            "Epoch 79/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1684 - accuracy: 0.7448\n",
            "Epoch 80/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1684 - accuracy: 0.7569\n",
            "Epoch 81/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1679 - accuracy: 0.7483\n",
            "Epoch 82/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1676 - accuracy: 0.7500\n",
            "Epoch 83/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1677 - accuracy: 0.7517\n",
            "Epoch 84/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1667 - accuracy: 0.7483\n",
            "Epoch 85/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1666 - accuracy: 0.7465\n",
            "Epoch 86/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1660 - accuracy: 0.7500\n",
            "Epoch 87/150\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1658 - accuracy: 0.7552\n",
            "Epoch 88/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1659 - accuracy: 0.7535\n",
            "Epoch 89/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1653 - accuracy: 0.7483\n",
            "Epoch 90/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1655 - accuracy: 0.7465\n",
            "Epoch 91/150\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.1651 - accuracy: 0.7431\n",
            "Epoch 92/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1654 - accuracy: 0.7535\n",
            "Epoch 93/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1641 - accuracy: 0.7535\n",
            "Epoch 94/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1642 - accuracy: 0.7569\n",
            "Epoch 95/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1641 - accuracy: 0.7517\n",
            "Epoch 96/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1635 - accuracy: 0.7552\n",
            "Epoch 97/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1634 - accuracy: 0.7604\n",
            "Epoch 98/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1631 - accuracy: 0.7552\n",
            "Epoch 99/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1630 - accuracy: 0.7604\n",
            "Epoch 100/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1625 - accuracy: 0.7552\n",
            "Epoch 101/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1628 - accuracy: 0.7517\n",
            "Epoch 102/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1626 - accuracy: 0.7535\n",
            "Epoch 103/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1622 - accuracy: 0.7604\n",
            "Epoch 104/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1619 - accuracy: 0.7587\n",
            "Epoch 105/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1617 - accuracy: 0.7552\n",
            "Epoch 106/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.7587\n",
            "Epoch 107/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1612 - accuracy: 0.7622\n",
            "Epoch 108/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1610 - accuracy: 0.7639\n",
            "Epoch 109/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1612 - accuracy: 0.7552\n",
            "Epoch 110/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1610 - accuracy: 0.7622\n",
            "Epoch 111/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1607 - accuracy: 0.7604\n",
            "Epoch 112/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1606 - accuracy: 0.7656\n",
            "Epoch 113/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1602 - accuracy: 0.7639\n",
            "Epoch 114/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1609 - accuracy: 0.7656\n",
            "Epoch 115/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1602 - accuracy: 0.7604\n",
            "Epoch 116/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1599 - accuracy: 0.7604\n",
            "Epoch 117/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1602 - accuracy: 0.7656\n",
            "Epoch 118/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1597 - accuracy: 0.7691\n",
            "Epoch 119/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1591 - accuracy: 0.7622\n",
            "Epoch 120/150\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.1599 - accuracy: 0.7674\n",
            "Epoch 121/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1591 - accuracy: 0.7760\n",
            "Epoch 122/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1596 - accuracy: 0.7708\n",
            "Epoch 123/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1591 - accuracy: 0.7708\n",
            "Epoch 124/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1593 - accuracy: 0.7639\n",
            "Epoch 125/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1587 - accuracy: 0.7726\n",
            "Epoch 126/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1589 - accuracy: 0.7691\n",
            "Epoch 127/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1591 - accuracy: 0.7639\n",
            "Epoch 128/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1585 - accuracy: 0.7812\n",
            "Epoch 129/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1581 - accuracy: 0.7622\n",
            "Epoch 130/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1588 - accuracy: 0.7639\n",
            "Epoch 131/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1581 - accuracy: 0.7691\n",
            "Epoch 132/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1580 - accuracy: 0.7760\n",
            "Epoch 133/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1583 - accuracy: 0.7743\n",
            "Epoch 134/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1584 - accuracy: 0.7691\n",
            "Epoch 135/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1581 - accuracy: 0.7743\n",
            "Epoch 136/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1574 - accuracy: 0.7812\n",
            "Epoch 137/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1583 - accuracy: 0.7674\n",
            "Epoch 138/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1583 - accuracy: 0.7708\n",
            "Epoch 139/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1578 - accuracy: 0.7760\n",
            "Epoch 140/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1578 - accuracy: 0.7760\n",
            "Epoch 141/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1574 - accuracy: 0.7743\n",
            "Epoch 142/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1573 - accuracy: 0.7708\n",
            "Epoch 143/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1576 - accuracy: 0.7795\n",
            "Epoch 144/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1583 - accuracy: 0.7674\n",
            "Epoch 145/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1576 - accuracy: 0.7760\n",
            "Epoch 146/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1570 - accuracy: 0.7708\n",
            "Epoch 147/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1573 - accuracy: 0.7691\n",
            "Epoch 148/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1576 - accuracy: 0.7778\n",
            "Epoch 149/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1573 - accuracy: 0.7778\n",
            "Epoch 150/150\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.1572 - accuracy: 0.7743\n",
            "6/6 - 0s - loss: 0.1703 - accuracy: 0.7396 - 109ms/epoch - 18ms/step\n",
            "\n",
            "Test accuracy: 0.7395833134651184\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG,\n",
        "                    format='%(asctime)s %(message)s',\n",
        "                    handlers=[logging.FileHandler(\"gnn.log\"),\n",
        "                              logging.StreamHandler()])\n",
        "\n",
        "# Read Data\n",
        "df = pd.read_table('./diabetes.txt',header=None,encoding='gb2312',sep='\\t')\n",
        "df.astype(float)\n",
        "# remove redundant col which is the opposite value of the 10th col\n",
        "df.pop(10)\n",
        "# remove first col of bias = 1\n",
        "df.pop(0)\n",
        "# the label column\n",
        "label = df.pop(9)\n",
        "\n",
        "# train feature\n",
        "train_feature = df[:576]\n",
        "# train label\n",
        "train_label = label[:576]\n",
        "# test feature\n",
        "test_feature = df[576:]\n",
        "# test label\n",
        "test_label = label[576:]\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(6, input_shape=(8,), activation='relu', bias_initializer='ones', kernel_initializer='random_uniform'),\n",
        "    Dense(6, activation='relu'),\n",
        "    Dense(4, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001, rho=0.9),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "epochs = 150\n",
        "history = model.fit(train_feature.values, train_label.values, epochs=epochs)\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_feature,  test_label, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the experiment above, we can see that the stochastic gradient descent method indeed has a problem of converging to the global minimum, resulting in an accuracy lower than other training methods. On the other hand, the genetic algorithm performs well since it has the selection, crossover, and mutation mechanisms that could improve the local minimum dilemma by producing variant kinds of genes (weighted matrix) and selecting the best among them."
      ],
      "metadata": {
        "id": "U8Vc7FYrIen2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANN Optimisation - Genetic Algorithm**"
      ],
      "metadata": {
        "id": "QJ-89k1eor5J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZVcMmxRtDiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b421013-83fe-4d57-9d30-128e4daa3f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 4ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "6/6 [==============================] - 0s 2ms/step\n",
            "Test Accuracy: 0.64\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG,\n",
        "                    format='%(asctime)s %(message)s',\n",
        "                    handlers=[logging.FileHandler(\"ann_test.log\"),\n",
        "                              logging.StreamHandler()])\n",
        "\n",
        "class ANN(Sequential):\n",
        "\n",
        "    def __init__(self, child_weights=None):\n",
        "        super().__init__()\n",
        "\n",
        "        if child_weights is None:\n",
        "            layer1 = Dense(6, input_shape=(8,), activation='relu')\n",
        "            layer2 = Dense(6, activation='relu')\n",
        "            layer3 = Dense(4, activation='relu')\n",
        "            layer4 = Dense(1, activation='linear')\n",
        "            self.add(layer1)\n",
        "            self.add(layer2)\n",
        "            self.add(layer3)\n",
        "            self.add(layer4)\n",
        "        else:\n",
        "            self.add(\n",
        "                Dense(\n",
        "                    6,\n",
        "                    input_shape=(8,),\n",
        "                    activation='relu',\n",
        "                    weights=[child_weights[0], np.ones(6)])\n",
        "                )\n",
        "            self.add(\n",
        "                Dense(\n",
        "                    6,\n",
        "                    activation='relu',\n",
        "                    weights=[child_weights[1], np.zeros(6)])\n",
        "            )\n",
        "            self.add(\n",
        "                Dense(\n",
        "                    4,\n",
        "                    activation='relu',\n",
        "                    weights=[child_weights[2], np.zeros(4)])\n",
        "            )\n",
        "            self.add(\n",
        "                Dense(\n",
        "                    1,\n",
        "                    activation='linear',\n",
        "                    weights=[child_weights[3], np.zeros(1)])\n",
        "            )\n",
        "\n",
        "    def forward_propagation(self, train_feature, train_label):\n",
        "        predict_label = self.predict(train_feature.values)\n",
        "        self.fitness = accuracy_score(train_label, predict_label.round())\n",
        "\n",
        "def crossover(nn1, nn2):\n",
        "\n",
        "    nn1_weights = []\n",
        "    nn2_weights = []\n",
        "    child_weights = []\n",
        "\n",
        "    for layer in nn1.layers:\n",
        "        nn1_weights.append(layer.get_weights()[0])\n",
        "\n",
        "    for layer in nn2.layers:\n",
        "        nn2_weights.append(layer.get_weights()[0])\n",
        "\n",
        "    for i in range(len(nn1_weights)):\n",
        "        # Get single point to split the matrix in parents based on # of cols\n",
        "        split = random.randint(0, np.shape(nn1_weights[i])[1]-1)\n",
        "        # Iterate through after a single point and set the remaing cols to nn_2\n",
        "        for j in range(split, np.shape(nn1_weights[i])[1]-1):\n",
        "            nn1_weights[i][:, j] = nn2_weights[i][:, j]\n",
        "\n",
        "        child_weights.append(nn1_weights[i])\n",
        "\n",
        "    mutation(child_weights)\n",
        "\n",
        "    child = ANN(child_weights)\n",
        "    return child\n",
        "\n",
        "def mutation(child_weights):\n",
        "    selection = random.randint(0, len(child_weights)-1)\n",
        "    mut = random.uniform(0, 1)\n",
        "    if mut <= .05:\n",
        "        child_weights[selection] *= random.randint(2, 5)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "# Preprocess Data\n",
        "df = pd.read_table('./diabetes.txt',header=None,encoding='gb2312',sep='\\t')\n",
        "df.astype(float)\n",
        "# remove redundant col which is the opposite value of the 10th col\n",
        "df.pop(10)\n",
        "# remove first col of bias = 1\n",
        "df.pop(0)\n",
        "# the label column\n",
        "label = df.pop(9)\n",
        "# train feature\n",
        "train_feature = df[:576]\n",
        "# train label\n",
        "train_label = label[:576]\n",
        "# test feature\n",
        "test_feature = df[576:]\n",
        "# test label\n",
        "test_label = label[576:]\n",
        "\n",
        "# store all active ANNs\n",
        "networks = []\n",
        "pool = []\n",
        "# Generation counter\n",
        "generation = 0\n",
        "# Initial Population\n",
        "population = 10\n",
        "for i in range(population):\n",
        "    networks.append(ANN())\n",
        "# Track Max Fitness\n",
        "max_fitness = 0\n",
        "# Store Max Fitness Weights\n",
        "optimal_weights = []\n",
        "\n",
        "epochs = 10\n",
        "# Evolution Loop\n",
        "for i in range(epochs):\n",
        "    generation += 1\n",
        "    logging.debug(\"Generation: \" + str(generation) + \"\\r\\n\")\n",
        "\n",
        "    for ann in networks:\n",
        "        # Propagate to calculate fitness score\n",
        "        ann.forward_propagation(train_feature, train_label)\n",
        "        # Add to pool after calculating fitness\n",
        "        pool.append(ann)\n",
        "\n",
        "    # Clear for propagation of next children\n",
        "    networks.clear()\n",
        "\n",
        "    # Sort anns by fitness\n",
        "    pool = sorted(pool, key=lambda x: x.fitness)\n",
        "    pool.reverse()\n",
        "\n",
        "    # Find Max Fitness and Log Associated Weights\n",
        "    for i in range(len(pool)):\n",
        "        if pool[i].fitness > max_fitness:\n",
        "            max_fitness = pool[i].fitness\n",
        "\n",
        "            logging.debug(\"Max Fitness: \" + str(max_fitness) + \"\\r\\n\")\n",
        "\n",
        "            # Iterate through layers, get weights, and append to optimal\n",
        "            optimal_weights = []\n",
        "            for layer in pool[i].layers:\n",
        "                optimal_weights.append(layer.get_weights()[0])\n",
        "            logging.debug('optimal_weights: ' + str(optimal_weights)+\"\\r\\n\")\n",
        "\n",
        "    # Crossover: top 5 randomly select 2 partners\n",
        "    for i in range(5):\n",
        "        for j in range(2):\n",
        "            # Create a child and add to networks\n",
        "            temp = crossover(pool[i], random.choice(pool))\n",
        "            # Add to networks to calculate fitness score next iteration\n",
        "            networks.append(temp)\n",
        "\n",
        "# Create a Genetic Neural Network with optimal initial weights\n",
        "ann = ANN(optimal_weights)\n",
        "predict_label = ann.predict(test_feature.values)\n",
        "print('Test Accuracy: %.2f' % accuracy_score(test_label, predict_label.round()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters tuning and discoveries!!"
      ],
      "metadata": {
        "id": "hbkVS55txWF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some factors may affect the genetic algorithm-based ANN modelâ€™s performance, such as the number of layers, number of neurons in one layer, population size, and the number of generations. Therefore, I ran through some training to discover the performance of the model with different parameters.\n",
        "\n",
        "Here to simplify the expression,\n",
        "\n",
        "\n",
        "*   iL = input layer\n",
        "*   hL = hidden layer(s)\n",
        "\n",
        "*   oL = output layer\n",
        "*   n = number of neurons in one layer\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cw-2JfBYOidv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tune the number of neurons"
      ],
      "metadata": {
        "id": "7i94QCoLO5uN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These training and testing were using genetic algorithms with 20 initial populations and were run in 200 generations"
      ],
      "metadata": {
        "id": "0l_SYv3kO_6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Define the table data\n",
        "table_data = [\n",
        "    [\"Model\", \"Max fitness (accuracy)\", \"Testing accuracy\"],\n",
        "    [\"iL-> 1hL (2n) -> OL\", \"0.67\", \"0.64\"],\n",
        "    [\"iL-> 1hL (4n) -> OL\", \"0.70\", \"0.68\"],\n",
        "    [\"iL-> 1hL (6n) -> OL\", \"0.67\", \"0.67\"],\n",
        "    [\"iL-> 1hL (8n) -> OL\", \"0.71\", \"0.70\"],\n",
        "]\n",
        "\n",
        "# Print the table using tabulate\n",
        "print(tabulate(table_data, headers=\"firstrow\", tablefmt=\"grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "u8D5LigsWAac",
        "outputId": "f09a031d-6a70-43d7-9e22-79262350ca9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+--------------------------+--------------------+\n",
            "| Model               |   Max fitness (accuracy) |   Testing accuracy |\n",
            "+=====================+==========================+====================+\n",
            "| iL-> 1hL (2n) -> OL |                     0.67 |               0.64 |\n",
            "+---------------------+--------------------------+--------------------+\n",
            "| iL-> 1hL (4n) -> OL |                     0.7  |               0.68 |\n",
            "+---------------------+--------------------------+--------------------+\n",
            "| iL-> 1hL (6n) -> OL |                     0.67 |               0.67 |\n",
            "+---------------------+--------------------------+--------------------+\n",
            "| iL-> 1hL (8n) -> OL |                     0.71 |               0.7  |\n",
            "+---------------------+--------------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From table 1, it is obvious that with all parameters remain the same but only tune the number of neurons in the only hidden layer, the more neurons in the layer have higher accuracy in classifying the unseen data. Thus, although the testing accuracy and maximum fitness are not strictly increasing along with the increased number of neurons in the hidden layer, it shows a clear trend in the number of neurons in a layer and accuracy rate."
      ],
      "metadata": {
        "id": "SKl0z-JYPElS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Increase the number of layers and tune the number of neurons in each layer"
      ],
      "metadata": {
        "id": "QQQNKldgS_V1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These training and testing were using genetic algorithms with 20 initial populations and were run in 200 generations.\n",
        "\n"
      ],
      "metadata": {
        "id": "oJlwyOvcTLnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Define the table data\n",
        "table_data = [\n",
        "    [\"Model\", \"Max fitness (accuracy)\", \"Testing accuracy\"],\n",
        "    [\"iL-> 2hL (8n, 8n) - > OL\", \"0.75\", \"0.69\"],\n",
        "    [\"iL-> 1hL (8n, 16n) - > OL\", \"0.76\", \"0.73\"],\n",
        "    [\"iL-> 1hL (16n, 16n) - > OL\", \"0.75\", \"0.676\"],\n",
        "\n",
        "]\n",
        "\n",
        "# Print the table using tabulate\n",
        "print(tabulate(table_data, headers=\"firstrow\", tablefmt=\"grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "h2587XrcUp7O",
        "outputId": "7ffab86b-fdef-424b-d006-d0b1a6bf973c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------+--------------------------+--------------------+\n",
            "| Model                      |   Max fitness (accuracy) |   Testing accuracy |\n",
            "+============================+==========================+====================+\n",
            "| iL-> 2hL (8n, 8n) - > OL   |                     0.75 |              0.69  |\n",
            "+----------------------------+--------------------------+--------------------+\n",
            "| iL-> 1hL (8n, 16n) - > OL  |                     0.76 |              0.73  |\n",
            "+----------------------------+--------------------------+--------------------+\n",
            "| iL-> 1hL (16n, 16n) - > OL |                     0.75 |              0.676 |\n",
            "+----------------------------+--------------------------+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to table 2, with more neurons in each layer, the testing accuracy gets higher. The result reinforces the discovery from table 1. Additionally, comparing table 1 and table 2, we can observe that more hidden layers can also raise the accuracy rate."
      ],
      "metadata": {
        "id": "eiYDDfYDW5Qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning the population size"
      ],
      "metadata": {
        "id": "0VTAQf67W8i_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From many trials, I discovered that turning the initial population size doesnâ€™t significantly impact the accuracy rate since the genetic algorithm mechanism will eventually tune the weighted matrix towards an optimal status."
      ],
      "metadata": {
        "id": "7BdqBeVvXCvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning the number of generations"
      ],
      "metadata": {
        "id": "LYY_f72YXGpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as tuning the population size of the gene pool, it is also hard to say that with more generations, the higher accuracy one will get. From many trials I trained the model with 10, 25, 50, 100 and 200 generations, sometimes 10 generations yield a better result (test accuracy = 0.72) than 50 generations (test accuracy = 0.64). however, according to the rule of thumbs, there is still a higher probability for models with a higher number of generations that generate a better-weighted matrix and yields a better accuracy score."
      ],
      "metadata": {
        "id": "0-QOqUD5XLMX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqRDYoEoqNry"
      },
      "source": [
        "# **Improving the current GA based ANN model**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, to achieve a higher accuracy apart from applying the genetic algorithm, I tried to attach an additional training for the ANN model, which is by applying the Adam optimizer with Mean squared error regression loss function to compile the ANN model with the optimal weight matrix. The followings are my discoveries:"
      ],
      "metadata": {
        "id": "2jyNhj5EyaVW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7meKrVfqTD1",
        "outputId": "b70006eb-b659-44e6-da79-8cf48156747c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "6/6 [==============================] - 0s 2ms/step\n",
            "Test Accuracy: 0.72\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG,\n",
        "                    format='%(asctime)s %(message)s',\n",
        "                    handlers=[logging.FileHandler(\"ga+adam.log\"),\n",
        "                              logging.StreamHandler()])\n",
        "\n",
        "class ANN(Sequential):\n",
        "\n",
        "    def __init__(self, child_weights=None):\n",
        "        super().__init__()\n",
        "\n",
        "        if child_weights is None:\n",
        "            layer1 = Dense(6, input_shape=(8,), activation='relu')\n",
        "            layer2 = Dense(6, activation='relu')\n",
        "            layer3 = Dense(4, activation='relu')\n",
        "            layer4 = Dense(1, activation='linear')\n",
        "            self.add(layer1)\n",
        "            self.add(layer2)\n",
        "            self.add(layer3)\n",
        "            self.add(layer4)\n",
        "        else:\n",
        "            self.add(\n",
        "                Dense(\n",
        "                    6,\n",
        "                    input_shape=(8,),\n",
        "                    activation='relu',\n",
        "                    weights=[child_weights[0], np.ones(6)])\n",
        "                )\n",
        "            self.add(\n",
        "                Dense(\n",
        "                    6,\n",
        "                    activation='relu',\n",
        "                    weights=[child_weights[1], np.zeros(6)])\n",
        "            )\n",
        "            self.add(\n",
        "                Dense(\n",
        "                    4,\n",
        "                    activation='relu',\n",
        "                    weights=[child_weights[2], np.zeros(4)])\n",
        "            )\n",
        "            self.add(\n",
        "                Dense(\n",
        "                    1,\n",
        "                    activation='linear',\n",
        "                    weights=[child_weights[3], np.zeros(1)])\n",
        "            )\n",
        "\n",
        "    def forward_propagation(self, train_feature, train_label):\n",
        "        predict_label = self.predict(train_feature.values)\n",
        "        self.fitness = accuracy_score(train_label, predict_label.round())\n",
        "\n",
        "    def compile_train(self, train_feature, train_label, epochs):\n",
        "        self.compile(\n",
        "                      optimizer=optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),\n",
        "                      loss='mean_squared_error',\n",
        "                      metrics=['accuracy']\n",
        "                      )\n",
        "        #epochs = 5000\n",
        "        self.fit(train_feature.values, train_label.values, epochs=epochs)\n",
        "\n",
        "def crossover(nn1, nn2):\n",
        "\n",
        "    nn1_weights = []\n",
        "    nn2_weights = []\n",
        "    child_weights = []\n",
        "\n",
        "    for layer in nn1.layers:\n",
        "        nn1_weights.append(layer.get_weights()[0])\n",
        "\n",
        "    for layer in nn2.layers:\n",
        "        nn2_weights.append(layer.get_weights()[0])\n",
        "\n",
        "    for i in range(len(nn1_weights)):\n",
        "        # Get single point to split the matrix in parents based on # of cols\n",
        "        split = random.randint(0, np.shape(nn1_weights[i])[1]-1)\n",
        "        # Iterate through after a single point and set the remaing cols to nn_2\n",
        "        for j in range(split, np.shape(nn1_weights[i])[1]-1):\n",
        "            nn1_weights[i][:, j] = nn2_weights[i][:, j]\n",
        "\n",
        "        child_weights.append(nn1_weights[i])\n",
        "\n",
        "    mutation(child_weights)\n",
        "\n",
        "    child = ANN(child_weights)\n",
        "    return child\n",
        "\n",
        "def mutation(child_weights):\n",
        "    selection = random.randint(0, len(child_weights)-1)\n",
        "    mut = random.uniform(0, 1)\n",
        "    if mut <= .05:\n",
        "        child_weights[selection] *= random.randint(2, 5)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "# Preprocess Data\n",
        "df = pd.read_table('./diabetes.txt',header=None,encoding='gb2312',sep='\\t')\n",
        "df.astype(float)\n",
        "# remove redundant col which is the opposite value of the 10th col\n",
        "df.pop(10)\n",
        "# remove first col of bias = 1\n",
        "df.pop(0)\n",
        "# the label column\n",
        "label = df.pop(9)\n",
        "# train feature\n",
        "train_feature = df[:576]\n",
        "# train label\n",
        "train_label = label[:576]\n",
        "# test feature\n",
        "test_feature = df[576:]\n",
        "# test label\n",
        "test_label = label[576:]\n",
        "\n",
        "# store all active ANNs\n",
        "networks = []\n",
        "pool = []\n",
        "# Generation counter\n",
        "generation = 0\n",
        "# Initial Population\n",
        "population = 10\n",
        "for i in range(population):\n",
        "    networks.append(ANN())\n",
        "# Track Max Fitness\n",
        "max_fitness = 0\n",
        "# Store Max Fitness Weights\n",
        "optimal_weights = []\n",
        "\n",
        "epochs = 10\n",
        "# Evolution Loop\n",
        "for i in range(epochs):\n",
        "    generation += 1\n",
        "    logging.debug(\"Generation: \" + str(generation) + \"\\r\\n\")\n",
        "\n",
        "    for ann in networks:\n",
        "        # Propagate to calculate fitness score\n",
        "        ann.forward_propagation(train_feature, train_label)\n",
        "        # Add to pool after calculating fitness\n",
        "        pool.append(ann)\n",
        "\n",
        "    # Clear for propagation of next children\n",
        "    networks.clear()\n",
        "\n",
        "    # Sort anns by fitness\n",
        "    pool = sorted(pool, key=lambda x: x.fitness)\n",
        "    pool.reverse()\n",
        "\n",
        "    # Find Max Fitness and Log Associated Weights\n",
        "    for i in range(len(pool)):\n",
        "        if pool[i].fitness > max_fitness:\n",
        "            max_fitness = pool[i].fitness\n",
        "\n",
        "            logging.debug(\"Max Fitness: \" + str(max_fitness) + \"\\r\\n\")\n",
        "\n",
        "            # Iterate through layers, get weights, and append to optimal\n",
        "            optimal_weights = []\n",
        "            for layer in pool[i].layers:\n",
        "                optimal_weights.append(layer.get_weights()[0])\n",
        "            logging.debug('optimal_weights: ' + str(optimal_weights)+\"\\r\\n\")\n",
        "\n",
        "    # Crossover: top 5 randomly select 2 partners\n",
        "    for i in range(5):\n",
        "        for j in range(2):\n",
        "            # Create a child and add to networks\n",
        "            temp = crossover(pool[i], random.choice(pool))\n",
        "            # Add to networks to calculate fitness score next iteration\n",
        "            networks.append(temp)\n",
        "\n",
        "# Create a Genetic Neural Network with optimal initial weights\n",
        "ann = ANN(optimal_weights)\n",
        "predict_label = ann.predict(test_feature.values)\n",
        "print('Test Accuracy: %.2f' % accuracy_score(test_label, predict_label.round()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG,\n",
        "                    format='%(asctime)s %(message)s',\n",
        "                    handlers=[logging.FileHandler(\"ga+rmsp.log\"),\n",
        "                              logging.StreamHandler()])\n",
        "\n",
        "class ANN(Sequential):\n",
        "\n",
        "    def __init__(self, child_weights=None):\n",
        "        super().__init__()\n",
        "\n",
        "        if child_weights is None:\n",
        "            layer1 = Dense(6, input_shape=(8,), activation='relu')\n",
        "            layer2 = Dense(6, activation='relu')\n",
        "            layer3 = Dense(4, activation='relu')\n",
        "            layer4 = Dense(1, activation='linear')\n",
        "            self.add(layer1)\n",
        "            self.add(layer2)\n",
        "            self.add(layer3)\n",
        "            self.add(layer4)\n",
        "        else:\n",
        "            self.add(\n",
        "                Dense(\n",
        "                    6,\n",
        "                    input_shape=(8,),\n",
        "                    activation='relu',\n",
        "                    weights=[child_weights[0], np.ones(6)])\n",
        "                )\n",
        "            self.add(\n",
        "                Dense(\n",
        "                    6,\n",
        "                    activation='relu',\n",
        "                    weights=[child_weights[1], np.zeros(6)])\n",
        "            )\n",
        "            self.add(\n",
        "                Dense(\n",
        "                    4,\n",
        "                    activation='relu',\n",
        "                    weights=[child_weights[2], np.zeros(4)])\n",
        "            )\n",
        "            self.add(\n",
        "                Dense(\n",
        "                    1,\n",
        "                    activation='linear',\n",
        "                    weights=[child_weights[3], np.zeros(1)])\n",
        "            )\n",
        "\n",
        "    def forward_propagation(self, train_feature, train_label):\n",
        "        predict_label = self.predict(train_feature.values)\n",
        "        self.fitness = accuracy_score(train_label, predict_label.round())\n",
        "\n",
        "    def compile_train(self, train_feature, train_label, epochs):\n",
        "        self.compile(optimizer=optimizers.RMSprop(learning_rate=0.001, rho=0.9),\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "        self.fit(train_feature.values, train_label.values, epochs=epochs)\n",
        "\n",
        "def crossover(nn1, nn2):\n",
        "\n",
        "    nn1_weights = []\n",
        "    nn2_weights = []\n",
        "    child_weights = []\n",
        "\n",
        "    for layer in nn1.layers:\n",
        "        nn1_weights.append(layer.get_weights()[0])\n",
        "\n",
        "    for layer in nn2.layers:\n",
        "        nn2_weights.append(layer.get_weights()[0])\n",
        "\n",
        "    for i in range(len(nn1_weights)):\n",
        "        # Get single point to split the matrix in parents based on # of cols\n",
        "        split = random.randint(0, np.shape(nn1_weights[i])[1]-1)\n",
        "        # Iterate through after a single point and set the remaing cols to nn_2\n",
        "        for j in range(split, np.shape(nn1_weights[i])[1]-1):\n",
        "            nn1_weights[i][:, j] = nn2_weights[i][:, j]\n",
        "\n",
        "        child_weights.append(nn1_weights[i])\n",
        "\n",
        "    mutation(child_weights)\n",
        "\n",
        "    child = ANN(child_weights)\n",
        "    return child\n",
        "\n",
        "def mutation(child_weights):\n",
        "    selection = random.randint(0, len(child_weights)-1)\n",
        "    mut = random.uniform(0, 1)\n",
        "    if mut <= .05:\n",
        "        child_weights[selection] *= random.randint(2, 5)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "# Preprocess Data\n",
        "df = pd.read_table('./diabetes.txt',header=None,encoding='gb2312',sep='\\t')\n",
        "df.astype(float)\n",
        "# remove redundant col which is the opposite value of the 10th col\n",
        "df.pop(10)\n",
        "# remove first col of bias = 1\n",
        "df.pop(0)\n",
        "# the label column\n",
        "label = df.pop(9)\n",
        "# train feature\n",
        "train_feature = df[:576]\n",
        "# train label\n",
        "train_label = label[:576]\n",
        "# test feature\n",
        "test_feature = df[576:]\n",
        "# test label\n",
        "test_label = label[576:]\n",
        "\n",
        "# store all active ANNs\n",
        "networks = []\n",
        "pool = []\n",
        "# Generation counter\n",
        "generation = 0\n",
        "# Initial Population\n",
        "population = 10\n",
        "for i in range(population):\n",
        "    networks.append(ANN())\n",
        "# Track Max Fitness\n",
        "max_fitness = 0\n",
        "# Store Max Fitness Weights\n",
        "optimal_weights = []\n",
        "\n",
        "epochs = 10\n",
        "# Evolution Loop\n",
        "for i in range(epochs):\n",
        "    generation += 1\n",
        "    logging.debug(\"Generation: \" + str(generation) + \"\\r\\n\")\n",
        "\n",
        "    for ann in networks:\n",
        "        # Propagate to calculate fitness score\n",
        "        ann.forward_propagation(train_feature, train_label)\n",
        "        # Add to pool after calculating fitness\n",
        "        pool.append(ann)\n",
        "\n",
        "    # Clear for propagation of next children\n",
        "    networks.clear()\n",
        "\n",
        "    # Sort anns by fitness\n",
        "    pool = sorted(pool, key=lambda x: x.fitness)\n",
        "    pool.reverse()\n",
        "\n",
        "    # Find Max Fitness and Log Associated Weights\n",
        "    for i in range(len(pool)):\n",
        "        if pool[i].fitness > max_fitness:\n",
        "            max_fitness = pool[i].fitness\n",
        "\n",
        "            logging.debug(\"Max Fitness: \" + str(max_fitness) + \"\\r\\n\")\n",
        "\n",
        "            # Iterate through layers, get weights, and append to optimal\n",
        "            optimal_weights = []\n",
        "            for layer in pool[i].layers:\n",
        "                optimal_weights.append(layer.get_weights()[0])\n",
        "            logging.debug('optimal_weights: ' + str(optimal_weights)+\"\\r\\n\")\n",
        "\n",
        "    # Crossover: top 5 randomly select 2 partners\n",
        "    for i in range(5):\n",
        "        for j in range(2):\n",
        "            # Create a child and add to networks\n",
        "            temp = crossover(pool[i], random.choice(pool))\n",
        "            # Add to networks to calculate fitness score next iteration\n",
        "            networks.append(temp)\n",
        "\n",
        "# Create a Genetic Neural Network with optimal initial weights\n",
        "ann = ANN(optimal_weights)\n",
        "predict_label = ann.predict(test_feature.values)\n",
        "print('Test Accuracy: %.2f' % accuracy_score(test_label, predict_label.round()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG4ZixSwy3jB",
        "outputId": "92b9b83c-2b86-41b4-816a-2123ff745c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 3ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 1s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 1ms/step\n",
            "18/18 [==============================] - 0s 2ms/step\n",
            "6/6 [==============================] - 0s 2ms/step\n",
            "Test Accuracy: 0.70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "ob5nPeb5yujq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining Genetic Algorithms (GA) with Artificial Neural Networks (ANNs) and optimizers presents a compelling strategy for tackling intricate optimization problems. GA's ability to explore a vast search space and its robustness against local optima can lead to superior ANN configurations compared to traditional optimizers. Moreover, GA can potentially handle feature selection implicitly through the fitness function design. However, the computational cost associated with training multiple ANNs within the GA framework and the need for meticulous parameter tuning for both GA and ANNs are noteworthy challenges. Additionally, the effectiveness of this approach hinges on a problem-specific design, particularly regarding the fitness function and GA operators. Overall, GA-ANN systems with well-chosen optimizers offer a promising approach, but careful consideration of its complexities is essential for successful implementation."
      ],
      "metadata": {
        "id": "pVPI1iOLyymF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Future and Scope"
      ],
      "metadata": {
        "id": "JgeyhwdJxTF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The future scope of our proposed work can be carried\n",
        "out using some new hybrid techniques that incorporate\n",
        "wavelet transform, feature extraction, feature selection,\n",
        "Adaptive Neuro Fuzzy Inference, or wavelet techniques\n",
        "with Genetic Algorithm. To improve the accuracy of the\n",
        "results, various tuning parameters such as learning rate,\n",
        "momentum, and so on can be combined with GANN."
      ],
      "metadata": {
        "id": "4NOnMT-oxguM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1IqrdIyxfDvACDhNYa4ClirW3oq1FYz5e",
      "authorship_tag": "ABX9TyNLwfXQxzPoGwBoRYgvj69V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}